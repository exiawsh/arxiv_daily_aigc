[
    {
        "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model",
        "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.",
        "url": "http://arxiv.org/abs/2512.13604v1",
        "published_date": "2025-12-15T17:59:58+00:00",
        "updated_date": "2025-12-15T17:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianxiong Gao",
            "Zhaoxi Chen",
            "Xian Liu",
            "Junhao Zhuang",
            "Chengming Xu",
            "Jianfeng Feng",
            "Yu Qiao",
            "Yanwei Fu",
            "Chenyang Si",
            "Ziwei Liu"
        ],
        "tldr": "LongVie 2 is a novel end-to-end autoregressive framework for controllable, high-quality, and temporally consistent ultra-long video generation, outperforming state-of-the-art methods and achieving up to 5-minute video generation.",
        "tldr_zh": "LongVie 2 是一种新型端到端自回归框架，用于可控、高质量和时间一致的超长视频生成，优于现有技术，并可实现长达5分钟的视频生成。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence",
        "summary": "Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.",
        "url": "http://arxiv.org/abs/2512.13465v1",
        "published_date": "2025-12-15T16:03:26+00:00",
        "updated_date": "2025-12-15T16:03:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruiyan Wang",
            "Teng Hu",
            "Kaihui Huang",
            "Zihan Su",
            "Ran Yi",
            "Lizhuang Ma"
        ],
        "tldr": "PoseAnything introduces a novel pose-guided video generation framework capable of handling arbitrary skeletal inputs for both human and non-human characters, featuring part-aware temporal coherence and decoupled subject/camera motion control, and includes a new dataset.",
        "tldr_zh": "PoseAnything 提出了一个新颖的姿势引导视频生成框架，能够处理人类和非人类角色的任意骨骼输入，具有部分感知的时间一致性和解耦的主题/相机运动控制，并包含一个新的数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Motus: A Unified Latent Action World Model",
        "summary": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
        "url": "http://arxiv.org/abs/2512.13030v1",
        "published_date": "2025-12-15T06:58:40+00:00",
        "updated_date": "2025-12-15T06:58:40+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Hongzhe Bi",
            "Hengkai Tan",
            "Shenghao Xie",
            "Zeyuan Wang",
            "Shuhe Huang",
            "Haitian Liu",
            "Ruowen Zhao",
            "Yao Feng",
            "Chendong Xiang",
            "Yinze Rong",
            "Hongyan Zhao",
            "Hanyu Liu",
            "Zhizhong Su",
            "Lei Ma",
            "Hang Su",
            "Jun Zhu"
        ],
        "tldr": "Motus introduces a unified latent action world model using a Mixture-of-Transformer architecture and a UniDiffuser-style scheduler to integrate understanding, video generation, and action, achieving state-of-the-art performance in simulation and real-world robotic tasks.",
        "tldr_zh": "Motus 提出了一种统一的潜在动作世界模型，该模型使用混合 Transformer 架构和 UniDiffuser 风格的调度器来整合理解、视频生成和动作，在模拟和现实世界的机器人任务中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
        "summary": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.",
        "url": "http://arxiv.org/abs/2512.13507v1",
        "published_date": "2025-12-15T16:36:52+00:00",
        "updated_date": "2025-12-15T16:36:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siyan Chen",
            "Yanfei Chen",
            "Ying Chen",
            "Zhuo Chen",
            "Feng Cheng",
            "Xuyan Chi",
            "Jian Cong",
            "Qinpeng Cui",
            "Qide Dong",
            "Junliang Fan",
            "Jing Fang",
            "Zetao Fang",
            "Chengjian Feng",
            "Han Feng",
            "Mingyuan Gao",
            "Yu Gao",
            "Qiushan Guo",
            "Boyang Hao",
            "Qingkai Hao",
            "Bibo He",
            "Qian He",
            "Tuyen Hoang",
            "Ruoqing Hu",
            "Xi Hu",
            "Weilin Huang",
            "Zhaoyang Huang",
            "Zhongyi Huang",
            "Siqi Jiang",
            "Wei Jiang",
            "Yunpu Jiang",
            "Zhuo Jiang",
            "Ashley Kim",
            "Jianan Kong",
            "Zhichao Lai",
            "Shanshan Lao",
            "Ai Li",
            "Feiya Li",
            "Gen Li",
            "Huixia Li",
            "JiaShi Li",
            "Liang Li",
            "Ming Li",
            "Tao Li",
            "Xian Li",
            "Xiaojie Li",
            "Xiaoyang Li",
            "Xingxing Li",
            "Yameng Li",
            "Yifu Li",
            "Yiying Li",
            "Chao Liang",
            "Ying Liang",
            "Zhiqiang Liang",
            "Wang Liao",
            "Yalin Liao",
            "Heng Lin",
            "Kengyu Lin",
            "Shanchuan Lin",
            "Xi Lin",
            "Zhijie Lin",
            "Feng Ling",
            "Fangfang Liu",
            "Gaohong Liu",
            "Jiawei Liu",
            "Jie Liu",
            "Shouda Liu",
            "Shu Liu",
            "Sichao Liu",
            "Songwei Liu",
            "Xin Liu",
            "Xue Liu",
            "Yibo Liu",
            "Zikun Liu",
            "Zuxi Liu",
            "Junlin Lyu",
            "Lecheng Lyu",
            "Qian Lyu",
            "Han Mu",
            "Xiaonan Nie",
            "Jingzhe Ning",
            "Xitong Pan",
            "Yanghua Peng",
            "Lianke Qin",
            "Xueqiong Qu",
            "Yuxi Ren",
            "Yuchen Shen",
            "Guang Shi",
            "Lei Shi",
            "Yan Song",
            "Yinglong Song",
            "Fan Sun",
            "Li Sun",
            "Renfei Sun",
            "Zeyu Sun",
            "Wenjing Tang",
            "Zirui Tao",
            "Feng Wang",
            "Furui Wang",
            "Jinran Wang",
            "Junkai Wang",
            "Ke Wang",
            "Kexin Wang",
            "Qingyi Wang",
            "Rui Wang",
            "Sen Wang",
            "Shuai Wang",
            "Tingru Wang",
            "Weichen Wang",
            "Xin Wang",
            "Yanhui Wang",
            "Yue Wang",
            "Yuping Wang",
            "Yuxuan Wang",
            "Ziyu Wang",
            "Guoqiang Wei",
            "Wanru Wei",
            "Di Wu",
            "Guohong Wu",
            "Hanjie Wu",
            "Jian Wu",
            "Jie Wu",
            "Ruolan Wu",
            "Xinglong Wu",
            "Yonghui Wu",
            "Ruiqi Xia",
            "Liang Xiang",
            "Fei Xiao",
            "XueFeng Xiao",
            "Pan Xie",
            "Shuangyi Xie",
            "Shuang Xu",
            "Jinlan Xue",
            "Bangbang Yang",
            "Ceyuan Yang",
            "Jiaqi Yang",
            "Runkai Yang",
            "Tao Yang",
            "Yang Yang",
            "Yihang Yang",
            "ZhiXian Yang",
            "Ziyan Yang",
            "Yifan Yao",
            "Zilyu Ye",
            "Bowen Yu",
            "Chujie Yuan",
            "Linxiao Yuan",
            "Sichun Zeng",
            "Weihong Zeng",
            "Xuejiao Zeng",
            "Yan Zeng",
            "Chuntao Zhang",
            "Heng Zhang",
            "Jingjie Zhang",
            "Kuo Zhang",
            "Liang Zhang",
            "Liying Zhang",
            "Manlin Zhang",
            "Ting Zhang",
            "Weida Zhang",
            "Xiaohe Zhang",
            "Xinyan Zhang",
            "Yan Zhang",
            "Yuan Zhang",
            "Zixiang Zhang",
            "Fengxuan Zhao",
            "Huating Zhao",
            "Yang Zhao",
            "Hao Zheng",
            "Jianbin Zheng",
            "Xiaozheng Zheng",
            "Yangyang Zheng",
            "Yijie Zheng",
            "Jiexin Zhou",
            "Kuan Zhu",
            "Shenhan Zhu",
            "Wenjia Zhu",
            "Benhui Zou",
            "Feilong Zuo"
        ],
        "tldr": "Seedance 1.5 pro is a new audio-visual generation foundation model that focuses on joint audio-video generation, achieving high synchronization and quality through a dual-branch Diffusion Transformer architecture and post-training optimizations like SFT and RLHF, with a 10x speedup in inference.",
        "tldr_zh": "Seedance 1.5 pro是一个新的音视频生成基础模型，专注于联合音视频生成，通过双分支扩散Transformer架构和SFT与RLHF等后训练优化，实现了高度同步和高质量，并在推理速度上提升了10倍。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation",
        "summary": "We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/",
        "url": "http://arxiv.org/abs/2512.13495v1",
        "published_date": "2025-12-15T16:25:56+00:00",
        "updated_date": "2025-12-15T16:25:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiangning Zhang",
            "Junwei Zhu",
            "Zhenye Gan",
            "Donghao Luo",
            "Chuming Lin",
            "Feifan Xu",
            "Xu Peng",
            "Jianlong Hu",
            "Yuansen Liu",
            "Yijia Hong",
            "Weijian Cao",
            "Han Feng",
            "Xu Chen",
            "Chencan Fu",
            "Keke He",
            "Xiaobin Hu",
            "Chengjie Wang"
        ],
        "tldr": "The paper introduces Soul, a multimodal framework for generating high-fidelity, long-term digital human animations from a single image, text, and audio, outperforming existing models in various quality metrics and boasting significant speedup. They also introduce a large-scale dataset for this task.",
        "tldr_zh": "该论文提出了Soul，一个多模态框架，用于从单张图像、文本和音频生成高保真、长期的数字人动画，在各种质量指标上优于现有模型，并具有显著的加速效果。他们还为此任务引入了一个大规模数据集。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\\times$",
        "summary": "Native 4K (2160$\\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\\textbf{T3}$ ($\\textbf{T}$ransform $\\textbf{T}$rained $\\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an \"attention pattern\" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\\uparrow$ VQA and +0.08$\\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\\times$. Project page at https://zhangzjn.github.io/projects/T3-Video",
        "url": "http://arxiv.org/abs/2512.13492v1",
        "published_date": "2025-12-15T16:25:39+00:00",
        "updated_date": "2025-12-15T16:25:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiangning Zhang",
            "Junwei Zhu",
            "Teng Hu",
            "Yabiao Wang",
            "Donghao Luo",
            "Weijian Cao",
            "Zhenye Gan",
            "Xiaobin Hu",
            "Zhucun Xue",
            "Chengjie Wang"
        ],
        "tldr": "This paper introduces T3-Video, a method for accelerating native 4K video generation by over 10x by optimizing the forward logic of pretrained Transformer models without altering their core architecture, achieving performance improvements on 4K-VBench.",
        "tldr_zh": "本文介绍了一种名为T3-Video的方法，通过优化预训练Transformer模型的前向逻辑，在不改变其核心架构的前提下，将原生4K视频生成速度提高10倍以上，并在4K-VBench上实现了性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "KlingAvatar 2.0 Technical Report",
        "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.",
        "url": "http://arxiv.org/abs/2512.13313v1",
        "published_date": "2025-12-15T13:30:51+00:00",
        "updated_date": "2025-12-15T13:30:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kling Team",
            "Jialu Chen",
            "Yikang Ding",
            "Zhixue Fang",
            "Kun Gai",
            "Yuan Gao",
            "Kang He",
            "Jingyun Hua",
            "Boyuan Jiang",
            "Mingming Lao",
            "Xiaohan Li",
            "Hui Liu",
            "Jiwen Liu",
            "Xiaoqiang Liu",
            "Yuan Liu",
            "Shun Lu",
            "Yongsen Mao",
            "Yingchao Shao",
            "Huafeng Shi",
            "Xiaoyu Shi",
            "Peiqin Sun",
            "Songlin Tang",
            "Pengfei Wan",
            "Chao Wang",
            "Xuebo Wang",
            "Haoxian Zhang",
            "Yuanxing Zhang",
            "Yan Zhou"
        ],
        "tldr": "KlingAvatar 2.0 introduces a spatio-temporal cascade framework with modality-specific LLM experts for efficient, high-resolution, long-form avatar video generation with improved instruction following and identity preservation.",
        "tldr_zh": "KlingAvatar 2.0 引入了一个时空级联框架，结合模态特定的 LLM 专家，用于高效生成高分辨率、长格式的头像视频，同时提升了指令遵循和身份保持能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SneakPeek: Future-Guided Instructional Streaming Video Generation",
        "summary": "Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.",
        "url": "http://arxiv.org/abs/2512.13019v1",
        "published_date": "2025-12-15T06:32:57+00:00",
        "updated_date": "2025-12-15T06:32:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cheeun Hong",
            "German Barquero",
            "Fadime Sener",
            "Markos Georgopoulos",
            "Edgar Schönfeld",
            "Stefan Popov",
            "Yuming Du",
            "Oscar Mañas",
            "Albert Pumarola"
        ],
        "tldr": "The paper introduces SneakPeek, a diffusion-based autoregressive framework for generating temporally consistent and semantically faithful instructional videos from textual descriptions, addressing the challenges of long video generation with innovations in predictive causal adaptation, future-guided self-forcing, and multi-prompt conditioning.",
        "tldr_zh": "该论文介绍了 SneakPeek，一个基于扩散的自回归框架，用于从文本描述生成时间一致且语义准确的教学视频，通过预测因果适应、未来引导的自强化和多提示条件化来解决长视频生成的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]