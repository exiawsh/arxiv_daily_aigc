[
    {
        "title": "X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering",
        "summary": "We present X2Video, the first diffusion model for rendering photorealistic\nvideos guided by intrinsic channels including albedo, normal, roughness,\nmetallicity, and irradiance, while supporting intuitive multi-modal controls\nwith reference images and text prompts for both global and local regions. The\nintrinsic guidance allows accurate manipulation of color, material, geometry,\nand lighting, while reference images and text prompts provide intuitive\nadjustments in the absence of intrinsic information. To enable these\nfunctionalities, we extend the intrinsic-guided image generation model XRGB to\nvideo generation by employing a novel and efficient Hybrid Self-Attention,\nwhich ensures temporal consistency across video frames and also enhances\nfidelity to reference images. We further develop a Masked Cross-Attention to\ndisentangle global and local text prompts, applying them effectively onto\nrespective local and global regions. For generating long videos, our novel\nRecursive Sampling method incorporates progressive frame sampling, combining\nkeyframe prediction and frame interpolation to maintain long-range temporal\nconsistency while preventing error accumulation. To support the training of\nX2Video, we assembled a video dataset named InteriorVideo, featuring 1,154\nrooms from 295 interior scenes, complete with reliable ground-truth intrinsic\nchannel sequences and smooth camera trajectories. Both qualitative and\nquantitative evaluations demonstrate that X2Video can produce long, temporally\nconsistent, and photorealistic videos guided by intrinsic conditions.\nAdditionally, X2Video effectively accommodates multi-modal controls with\nreference images, global and local text prompts, and simultaneously supports\nediting on color, material, geometry, and lighting through parametric tuning.\nProject page: https://luckyhzt.github.io/x2video",
        "url": "http://arxiv.org/abs/2510.08530v1",
        "published_date": "2025-10-09T17:50:31+00:00",
        "updated_date": "2025-10-09T17:50:31+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "68U05",
            "I.3.3; I.3.6"
        ],
        "authors": [
            "Zhitong Huang",
            "Mohan Zhang",
            "Renhan Wang",
            "Rui Tang",
            "Hao Zhu",
            "Jing Liao"
        ],
        "tldr": "X2Video is a novel diffusion model for generating photorealistic and controllable videos guided by intrinsic channels, reference images, and text prompts, using a new hybrid attention mechanism and recursive sampling for temporal consistency.",
        "tldr_zh": "X2Video是一个新颖的扩散模型，通过内在通道、参考图像和文本提示引导，生成逼真且可控的视频。它使用了一种新的混合注意力机制和递归采样来保证时间一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning",
        "summary": "We introduce the task of arbitrary spatio-temporal video completion, where a\nvideo is generated from arbitrary, user-specified patches placed at any spatial\nlocation and timestamp, akin to painting on a video canvas. This flexible\nformulation naturally unifies many existing controllable video generation\ntasks--including first-frame image-to-video, inpainting, extension, and\ninterpolation--under a single, cohesive paradigm. Realizing this vision,\nhowever, faces a fundamental obstacle in modern latent video diffusion models:\nthe temporal ambiguity introduced by causal VAEs, where multiple pixel frames\nare compressed into a single latent representation, making precise frame-level\nconditioning structurally difficult. We address this challenge with\nVideoCanvas, a novel framework that adapts the In-Context Conditioning (ICC)\nparadigm to this fine-grained control task with zero new parameters. We propose\na hybrid conditioning strategy that decouples spatial and temporal control:\nspatial placement is handled via zero-padding, while temporal alignment is\nachieved through Temporal RoPE Interpolation, which assigns each condition a\ncontinuous fractional position within the latent sequence. This resolves the\nVAE's temporal ambiguity and enables pixel-frame-aware control on a frozen\nbackbone. To evaluate this new capability, we develop VideoCanvasBench, the\nfirst benchmark for arbitrary spatio-temporal video completion, covering both\nintra-scene fidelity and inter-scene creativity. Experiments demonstrate that\nVideoCanvas significantly outperforms existing conditioning paradigms,\nestablishing a new state of the art in flexible and unified video generation.",
        "url": "http://arxiv.org/abs/2510.08555v1",
        "published_date": "2025-10-09T17:58:59+00:00",
        "updated_date": "2025-10-09T17:58:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minghong Cai",
            "Qiulin Wang",
            "Zongli Ye",
            "Wenze Liu",
            "Quande Liu",
            "Weicai Ye",
            "Xintao Wang",
            "Pengfei Wan",
            "Kun Gai",
            "Xiangyu Yue"
        ],
        "tldr": "The paper introduces VideoCanvas, a framework for arbitrary spatio-temporal video completion using In-Context Conditioning, addressing the limitations of causal VAEs in latent video diffusion models and achieving state-of-the-art results in unified video generation.",
        "tldr_zh": "该论文介绍了 VideoCanvas，一个用于任意时空视频补全的框架，它使用上下文条件，解决了潜在视频扩散模型中因果 VAE 的局限性，并在统一视频生成方面取得了最先进的成果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control",
        "summary": "We present FlexTraj, a framework for image-to-video generation with flexible\npoint trajectory control. FlexTraj introduces a unified point-based motion\nrepresentation that encodes each point with a segmentation ID, a temporally\nconsistent trajectory ID, and an optional color channel for appearance cues,\nenabling both dense and sparse trajectory control. Instead of injecting\ntrajectory conditions into the video generator through token concatenation or\nControlNet, FlexTraj employs an efficient sequence-concatenation scheme that\nachieves faster convergence, stronger controllability, and more efficient\ninference, while maintaining robustness under unaligned conditions. To train\nsuch a unified point trajectory-controlled video generator, FlexTraj adopts an\nannealing training strategy that gradually reduces reliance on complete\nsupervision and aligned condition. Experimental results demonstrate that\nFlexTraj enables multi-granularity, alignment-agnostic trajectory control for\nvideo generation, supporting various applications such as motion cloning,\ndrag-based image-to-video, motion interpolation, camera redirection, flexible\naction control and mesh animations.",
        "url": "http://arxiv.org/abs/2510.08527v1",
        "published_date": "2025-10-09T17:50:22+00:00",
        "updated_date": "2025-10-09T17:50:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiyuan Zhang",
            "Can Wang",
            "Dongdong Chen",
            "Jing Liao"
        ],
        "tldr": "FlexTraj introduces a point-based motion representation for image-to-video generation, offering flexible and efficient trajectory control through a novel sequence-concatenation scheme and annealing training strategy.",
        "tldr_zh": "FlexTraj 提出了一种基于点的运动表示方法，用于图像到视频的生成，通过一种新的序列连接方案和退火训练策略，实现了灵活高效的轨迹控制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency",
        "summary": "This work represents the first effort to scale up continuous-time consistency\ndistillation to general application-level image and video diffusion models.\nAlthough continuous-time consistency model (sCM) is theoretically principled\nand empirically powerful for accelerating academic-scale diffusion, its\napplicability to large-scale text-to-image and video tasks remains unclear due\nto infrastructure challenges in Jacobian-vector product (JVP) computation and\nthe limitations of standard evaluation benchmarks. We first develop a\nparallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on\nmodels with over 10 billion parameters and high-dimensional video tasks. Our\ninvestigation reveals fundamental quality limitations of sCM in fine-detail\ngeneration, which we attribute to error accumulation and the \"mode-covering\"\nnature of its forward-divergence objective. To remedy this, we propose the\nscore-regularized continuous-time consistency model (rCM), which incorporates\nscore distillation as a long-skip regularizer. This integration complements sCM\nwith the \"mode-seeking\" reverse divergence, effectively improving visual\nquality while maintaining high generation diversity. Validated on large-scale\nmodels (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM\nmatches or surpasses the state-of-the-art distillation method DMD2 on quality\nmetrics while offering notable advantages in diversity, all without GAN tuning\nor extensive hyperparameter searches. The distilled models generate\nhigh-fidelity samples in only $1\\sim4$ steps, accelerating diffusion sampling\nby $15\\times\\sim50\\times$. These results position rCM as a practical and\ntheoretically grounded framework for advancing large-scale diffusion\ndistillation.",
        "url": "http://arxiv.org/abs/2510.08431v1",
        "published_date": "2025-10-09T16:45:30+00:00",
        "updated_date": "2025-10-09T16:45:30+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Kaiwen Zheng",
            "Yuji Wang",
            "Qianli Ma",
            "Huayu Chen",
            "Jintao Zhang",
            "Yogesh Balaji",
            "Jianfei Chen",
            "Ming-Yu Liu",
            "Jun Zhu",
            "Qinsheng Zhang"
        ],
        "tldr": "The paper introduces score-regularized continuous-time consistency model (rCM) for large-scale diffusion distillation, addressing quality limitations of existing continuous-time consistency models and achieving state-of-the-art results on image and video generation with significant speedups.",
        "tldr_zh": "该论文介绍了用于大规模扩散蒸馏的分数正则化连续时间一致性模型（rCM），解决了现有连续时间一致性模型的质量限制，并在图像和视频生成方面取得了最先进的结果，并显著加速。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniVideo: Unified Understanding, Generation, and Editing for Videos",
        "summary": "Unified multimodal models have shown promising results in multimodal content\ngeneration and editing but remain largely limited to the image domain. In this\nwork, we present UniVideo, a versatile framework that extends unified modeling\nto the video domain. UniVideo adopts a dual-stream design, combining a\nMultimodal Large Language Model (MLLM) for instruction understanding with a\nMultimodal DiT (MMDiT) for video generation. This design enables accurate\ninterpretation of complex multimodal instructions while preserving visual\nconsistency. Built on this architecture, UniVideo unifies diverse video\ngeneration and editing tasks under a single multimodal instruction paradigm and\nis jointly trained across them. Extensive experiments demonstrate that UniVideo\nmatches or surpasses state-of-the-art task-specific baselines in\ntext/image-to-video generation, in-context video generation and in-context\nvideo editing. Notably, the unified design of UniVideo enables two forms of\ngeneralization. First, UniVideo supports task composition, such as combining\nediting with style transfer, by integrating multiple capabilities within a\nsingle instruction. Second, even without explicit training on free-form video\nediting, UniVideo transfers its editing capability from large-scale image\nediting data to this setting, handling unseen instructions such as\ngreen-screening characters or changing materials within a video. Beyond these\ncore capabilities, UniVideo also supports visual-prompt-based video generation,\nwhere the MLLM interprets visual prompts and guides the MMDiT during synthesis.\nTo foster future research, we will release our model and code.",
        "url": "http://arxiv.org/abs/2510.08377v1",
        "published_date": "2025-10-09T16:01:30+00:00",
        "updated_date": "2025-10-09T16:01:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cong Wei",
            "Quande Liu",
            "Zixuan Ye",
            "Qiulin Wang",
            "Xintao Wang",
            "Pengfei Wan",
            "Kun Gai",
            "Wenhu Chen"
        ],
        "tldr": "UniVideo is a unified framework for video understanding, generation, and editing using a dual-stream architecture with an MLLM and MMDiT, demonstrating strong performance and generalization capabilities.",
        "tldr_zh": "UniVideo是一个统一的视频理解、生成和编辑框架，它采用双流架构，结合了多模态大型语言模型（MLLM）和多模态扩散Transformer（MMDiT），展现出强大的性能和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "LinVideo: A Post-Training Framework towards O(n) Attention in Efficient Video Generation",
        "summary": "Video diffusion models (DMs) have enabled high-quality video synthesis.\nHowever, their computation costs scale quadratically with sequence length\nbecause self-attention has quadratic complexity. While linear attention lowers\nthe cost, fully replacing quadratic attention requires expensive pretraining\ndue to the limited expressiveness of linear attention and the complexity of\nspatiotemporal modeling in video generation. In this paper, we present\nLinVideo, an efficient data-free post-training framework that replaces a target\nnumber of self-attention modules with linear attention while preserving the\noriginal model's performance. First, we observe a significant disparity in the\nreplaceability of different layers. Instead of manual or heuristic choices, we\nframe layer selection as a binary classification problem and propose selective\ntransfer, which automatically and progressively converts layers to linear\nattention with minimal performance impact. Additionally, to overcome the\nineffectiveness and inefficiency of existing objectives for this transfer\nprocess, we introduce an anytime distribution matching (ADM) objective that\naligns the distributions of samples across any timestep along the sampling\ntrajectory. This objective is efficient and recovers model performance.\nExtensive experiments show that our method achieves a 1.25-2.00x speedup while\npreserving generation quality, and our 4-step distilled model further delivers\na 15.92x latency reduction with minimal visual quality drop.",
        "url": "http://arxiv.org/abs/2510.08318v1",
        "published_date": "2025-10-09T15:03:39+00:00",
        "updated_date": "2025-10-09T15:03:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yushi Huang",
            "Xingtong Ge",
            "Ruihao Gong",
            "Chengtao Lv",
            "Jun Zhang"
        ],
        "tldr": "LinVideo introduces a post-training framework for efficient video generation by selectively replacing quadratic attention modules with linear attention, achieving significant speedups with minimal quality loss using a novel anytime distribution matching objective.",
        "tldr_zh": "LinVideo 提出了一个后训练框架，用于高效视频生成，通过选择性地将二次注意力模块替换为线性注意力，使用一种新颖的随时分布匹配目标，实现了显著的加速，同时尽量减少质量损失。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Real-Time Motion-Controllable Autoregressive Video Diffusion",
        "summary": "Real-time motion-controllable video generation remains challenging due to the\ninherent latency of bidirectional diffusion models and the lack of effective\nautoregressive (AR) approaches. Existing AR video diffusion models are limited\nto simple control signals or text-to-video generation, and often suffer from\nquality degradation and motion artifacts in few-step generation. To address\nthese challenges, we propose AR-Drag, the first RL-enhanced few-step AR video\ndiffusion model for real-time image-to-video generation with diverse motion\ncontrol. We first fine-tune a base I2V model to support basic motion control,\nthen further improve it via reinforcement learning with a trajectory-based\nreward model. Our design preserves the Markov property through a Self-Rollout\nmechanism and accelerates training by selectively introducing stochasticity in\ndenoising steps. Extensive experiments demonstrate that AR-Drag achieves high\nvisual fidelity and precise motion alignment, significantly reducing latency\ncompared with state-of-the-art motion-controllable VDMs, while using only 1.3B\nparameters. Additional visualizations can be found on our project page:\nhttps://kesenzhao.github.io/AR-Drag.github.io/.",
        "url": "http://arxiv.org/abs/2510.08131v1",
        "published_date": "2025-10-09T12:17:11+00:00",
        "updated_date": "2025-10-09T12:17:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kesen Zhao",
            "Jiaxin Shi",
            "Beier Zhu",
            "Junbao Zhou",
            "Xiaolong Shen",
            "Yuan Zhou",
            "Qianru Sun",
            "Hanwang Zhang"
        ],
        "tldr": "The paper introduces AR-Drag, a reinforcement learning-enhanced autoregressive video diffusion model for real-time image-to-video generation with diverse motion control, achieving high fidelity and precise motion alignment with low latency.",
        "tldr_zh": "该论文介绍了 AR-Drag，一种通过强化学习增强的自回归视频扩散模型，用于具有多样化运动控制的实时图像到视频生成，以低延迟实现了高保真度和精确的运动对齐。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving",
        "summary": "Generative models have been widely applied to world modeling for environment\nsimulation and future state prediction. With advancements in autonomous\ndriving, there is a growing demand not only for high-fidelity video generation\nunder various controls, but also for producing diverse and meaningful\ninformation such as depth estimation. To address this, we propose CVD-STORM, a\ncross-view video diffusion model utilizing a spatial-temporal reconstruction\nVariational Autoencoder (VAE) that generates long-term, multi-view videos with\n4D reconstruction capabilities under various control inputs. Our approach first\nfine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its\nability to encode 3D structures and temporal dynamics. Subsequently, we\nintegrate this VAE into the video diffusion process to significantly improve\ngeneration quality. Experimental results demonstrate that our model achieves\nsubstantial improvements in both FID and FVD metrics. Additionally, the\njointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic\nscenes, providing valuable geometric information for comprehensive scene\nunderstanding.",
        "url": "http://arxiv.org/abs/2510.07944v1",
        "published_date": "2025-10-09T08:41:58+00:00",
        "updated_date": "2025-10-09T08:41:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianrui Zhang",
            "Yichen Liu",
            "Zilin Guo",
            "Yuxin Guo",
            "Jingcheng Ni",
            "Chenjing Ding",
            "Dan Xu",
            "Lewei Lu",
            "Zehuan Wu"
        ],
        "tldr": "The paper introduces CVD-STORM, a cross-view video diffusion model with a spatial-temporal reconstruction VAE for autonomous driving, capable of generating long-term, multi-view videos with 4D reconstruction under various controls.",
        "tldr_zh": "该论文介绍了CVD-STORM，一个具有时空重建VAE的跨视角视频扩散模型，用于自动驾驶，能够在各种控制下生成具有4D重建功能的长期多视角视频。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TTOM: Test-Time Optimization and Memorization for Compositional Video Generation",
        "summary": "Video Foundation Models (VFMs) exhibit remarkable visual generation\nperformance, but struggle in compositional scenarios (e.g., motion, numeracy,\nand spatial relation). In this work, we introduce Test-Time Optimization and\nMemorization (TTOM), a training-free framework that aligns VFM outputs with\nspatiotemporal layouts during inference for better text-image alignment. Rather\nthan direct intervention to latents or attention per-sample in existing work,\nwe integrate and optimize new parameters guided by a general layout-attention\nobjective. Furthermore, we formulate video generation within a streaming\nsetting, and maintain historical optimization contexts with a parametric memory\nmechanism that supports flexible operations, such as insert, read, update, and\ndelete. Notably, we found that TTOM disentangles compositional world knowledge,\nshowing powerful transferability and generalization. Experimental results on\nthe T2V-CompBench and Vbench benchmarks establish TTOM as an effective,\npractical, scalable, and efficient framework to achieve cross-modal alignment\nfor compositional video generation on the fly.",
        "url": "http://arxiv.org/abs/2510.07940v1",
        "published_date": "2025-10-09T08:37:00+00:00",
        "updated_date": "2025-10-09T08:37:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Leigang Qu",
            "Ziyang Wang",
            "Na Zheng",
            "Wenjie Wang",
            "Liqiang Nie",
            "Tat-Seng Chua"
        ],
        "tldr": "The paper introduces TTOM, a training-free framework for improving compositional video generation by aligning VFM outputs with spatiotemporal layouts during inference, using optimization and a parametric memory mechanism.",
        "tldr_zh": "该论文介绍了TTOM，一个无需训练的框架，通过在推理过程中将VFM输出与时空布局对齐，使用优化和参数化记忆机制来改进组合视频生成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Controllable Video Synthesis via Variational Inference",
        "summary": "Many video workflows benefit from a mixture of user controls with varying\ngranularity, from exact 4D object trajectories and camera paths to coarse text\nprompts, while existing video generative models are typically trained for fixed\ninput formats. We develop a video synthesis method that addresses this need and\ngenerates samples with high controllability for specified elements while\nmaintaining diversity for under-specified ones. We cast the task as variational\ninference to approximate a composed distribution, leveraging multiple video\ngeneration backbones to account for all task constraints collectively. To\naddress the optimization challenge, we break down the problem into step-wise KL\ndivergence minimization over an annealed sequence of distributions, and further\npropose a context-conditioned factorization technique that reduces modes in the\nsolution space to circumvent local optima. Experiments suggest that our method\nproduces samples with improved controllability, diversity, and 3D consistency\ncompared to prior works.",
        "url": "http://arxiv.org/abs/2510.07670v1",
        "published_date": "2025-10-09T01:48:16+00:00",
        "updated_date": "2025-10-09T01:48:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haoyi Duan",
            "Yunzhi Zhang",
            "Yilun Du",
            "Jiajun Wu"
        ],
        "tldr": "This paper introduces a video synthesis method using variational inference that allows for fine-grained user control over various aspects of the video while maintaining diversity in other areas. It addresses the challenge of combining different constraints through a step-wise optimization and context-conditioned factorization.",
        "tldr_zh": "本文提出了一种基于变分推断的视频合成方法，允许用户对视频的各个方面进行细粒度的控制，同时保持其他方面的多样性。它通过逐步优化和上下文条件分解来解决组合不同约束的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos",
        "summary": "Enabling robots to execute novel manipulation tasks zero-shot is a central\ngoal in robotics. Most existing methods assume in-distribution tasks or rely on\nfine-tuning with embodiment-matched data, limiting transfer across platforms.\nWe present NovaFlow, an autonomous manipulation framework that converts a task\ndescription into an actionable plan for a target robot without any\ndemonstrations. Given a task description, NovaFlow synthesizes a video using a\nvideo generation model and distills it into 3D actionable object flow using\noff-the-shelf perception modules. From the object flow, it computes relative\nposes for rigid objects and realizes them as robot actions via grasp proposals\nand trajectory optimization. For deformable objects, this flow serves as a\ntracking objective for model-based planning with a particle-based dynamics\nmodel. By decoupling task understanding from low-level control, NovaFlow\nnaturally transfers across embodiments. We validate on rigid, articulated, and\ndeformable object manipulation tasks using a table-top Franka arm and a Spot\nquadrupedal mobile robot, and achieve effective zero-shot execution without\ndemonstrations or embodiment-specific training. Project website:\nhttps://novaflow.lhy.xyz/.",
        "url": "http://arxiv.org/abs/2510.08568v1",
        "published_date": "2025-10-09T17:59:55+00:00",
        "updated_date": "2025-10-09T17:59:55+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Hongyu Li",
            "Lingfeng Sun",
            "Yafei Hu",
            "Duy Ta",
            "Jennifer Barry",
            "George Konidaris",
            "Jiahui Fu"
        ],
        "tldr": "NovaFlow achieves zero-shot robot manipulation by generating videos from task descriptions, extracting actionable object flow, and using it for robot control with both rigid and deformable objects, showing transferability across different robot embodiments.",
        "tldr_zh": "NovaFlow通过从任务描述生成视频、提取可操作的对象流，并将其用于刚性和柔性物体的机器人控制，实现了零样本机器人操作，并展示了在不同机器人形态之间的可转移性。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]