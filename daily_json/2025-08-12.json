[
    {
        "title": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation",
        "summary": "Current diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively.",
        "url": "http://arxiv.org/abs/2508.08248v1",
        "published_date": "2025-08-11T17:58:24+00:00",
        "updated_date": "2025-08-11T17:58:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuyuan Tu",
            "Yueming Pan",
            "Yinming Huang",
            "Xintong Han",
            "Zhen Xing",
            "Qi Dai",
            "Chong Luo",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "tldr": "StableAvatar is a novel end-to-end video diffusion transformer that synthesizes infinite-length, high-quality audio-driven avatar videos by addressing latent distribution drift through a time-step-aware audio adapter and an audio native guidance mechanism.",
        "tldr_zh": "StableAvatar 是一种新型的端到端视频扩散 Transformer，通过时间步感知音频适配器和音频原生引导机制解决潜在分布漂移问题，从而合成无限长度、高质量的音频驱动头像视频。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation",
        "summary": "Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io.",
        "url": "http://arxiv.org/abs/2508.08086v1",
        "published_date": "2025-08-11T15:29:57+00:00",
        "updated_date": "2025-08-11T15:29:57+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Zhongqi Yang",
            "Wenhang Ge",
            "Yuqi Li",
            "Jiaqi Chen",
            "Haoyuan Li",
            "Mengyin An",
            "Fei Kang",
            "Hua Xue",
            "Baixin Xu",
            "Yuyang Yin",
            "Eric Li",
            "Yang Liu",
            "Yikai Wang",
            "Hao-Xiang Guo",
            "Yahui Zhou"
        ],
        "tldr": "The paper introduces Matrix-3D, a framework for generating explorable 3D worlds from single images or text prompts using panoramic video generation and 3D reconstruction, along with a new large-scale panoramic video dataset.",
        "tldr_zh": "该论文介绍了Matrix-3D，一个利用全景视频生成和3D重建，从单个图像或文本提示生成可探索3D世界的框架，并提出了一个新的大型全景视频数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation",
        "summary": "Generating high-fidelity human videos that match user-specified identities is\nimportant yet challenging in the field of generative AI. Existing methods often\nrely on an excessive number of training parameters and lack compatibility with\nother AIGC tools. In this paper, we propose Stand-In, a lightweight and\nplug-and-play framework for identity preservation in video generation.\nSpecifically, we introduce a conditional image branch into the pre-trained\nvideo generation model. Identity control is achieved through restricted\nself-attentions with conditional position mapping, and can be learned quickly\nwith only 2000 pairs. Despite incorporating and training just $\\sim$1\\%\nadditional parameters, our framework achieves excellent results in video\nquality and identity preservation, outperforming other full-parameter training\nmethods. Moreover, our framework can be seamlessly integrated for other tasks,\nsuch as subject-driven video generation, pose-referenced video generation,\nstylization, and face swapping.",
        "url": "http://arxiv.org/abs/2508.07901v1",
        "published_date": "2025-08-11T12:17:38+00:00",
        "updated_date": "2025-08-11T12:17:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bowen Xue",
            "Qixin Yan",
            "Wenjing Wang",
            "Hao Liu",
            "Chen Li"
        ],
        "tldr": "The paper introduces Stand-In, a lightweight, plug-and-play framework for identity preservation in video generation that achieves state-of-the-art results with minimal parameter training and broad application potential.",
        "tldr_zh": "该论文介绍了 Stand-In，一个轻量级的、即插即用的视频生成身份保持框架，通过最少的参数训练实现了最先进的结果，并具有广泛的应用潜力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation",
        "summary": "The synthesis of spatiotemporally coherent 4D content presents fundamental\nchallenges in computer vision, requiring simultaneous modeling of high-fidelity\nspatial representations and physically plausible temporal dynamics. Current\napproaches often struggle to maintain view consistency while handling complex\nscene dynamics, particularly in large-scale environments with multiple\ninteracting elements. This work introduces Dream4D, a novel framework that\nbridges this gap through a synergy of controllable video generation and neural\n4D reconstruction. Our approach seamlessly combines a two-stage architecture:\nit first predicts optimal camera trajectories from a single image using\nfew-shot learning, then generates geometrically consistent multi-view sequences\nvia a specialized pose-conditioned diffusion process, which are finally\nconverted into a persistent 4D representation. This framework is the first to\nleverage both rich temporal priors from video diffusion models and geometric\nawareness of the reconstruction models, which significantly facilitates 4D\ngeneration and shows higher quality (e.g., mPSNR, mSSIM) over existing methods.",
        "url": "http://arxiv.org/abs/2508.07769v1",
        "published_date": "2025-08-11T08:55:47+00:00",
        "updated_date": "2025-08-11T08:55:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaoyan Liu",
            "Kangrui Li",
            "Jiaxin Liu"
        ],
        "tldr": "Dream4D introduces a novel framework for spatiotemporally consistent 4D content generation by combining camera-controlled video generation and neural 4D reconstruction, achieving higher quality compared to existing methods.",
        "tldr_zh": "Dream4D 提出了一种新颖的框架，通过结合相机控制的视频生成和神经 4D 重建来实现时空一致的 4D 内容生成，与现有方法相比，质量更高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ShoulderShot: Generating Over-the-Shoulder Dialogue Videos",
        "summary": "Over-the-shoulder dialogue videos are essential in films, short dramas, and\nadvertisements, providing visual variety and enhancing viewers' emotional\nconnection. Despite their importance, such dialogue scenes remain largely\nunderexplored in video generation research. The main challenges include\nmaintaining character consistency across different shots, creating a sense of\nspatial continuity, and generating long, multi-turn dialogues within limited\ncomputational budgets. Here, we present ShoulderShot, a framework that combines\ndual-shot generation with looping video, enabling extended dialogues while\npreserving character consistency. Our results demonstrate capabilities that\nsurpass existing methods in terms of shot-reverse-shot layout, spatial\ncontinuity, and flexibility in dialogue length, thereby opening up new\npossibilities for practical dialogue video generation. Videos and comparisons\nare available at https://shouldershot.github.io.",
        "url": "http://arxiv.org/abs/2508.07597v1",
        "published_date": "2025-08-11T03:56:23+00:00",
        "updated_date": "2025-08-11T03:56:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuang Zhang",
            "Junqi Cheng",
            "Haoyu Zhao",
            "Jiaxi Gu",
            "Fangyuan Zou",
            "Zenghui Lu",
            "Peng Shu"
        ],
        "tldr": "The paper introduces ShoulderShot, a framework for generating over-the-shoulder dialogue videos, addressing challenges like character consistency and spatial continuity in long, multi-turn dialogues.",
        "tldr_zh": "本文介绍了一个名为 ShoulderShot 的框架，用于生成过肩对话视频，解决了在长多轮对话中保持角色一致性和空间连续性等挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation",
        "summary": "Generating high-quality 4D content from monocular videos for applications\nsuch as digital humans and AR/VR poses challenges in ensuring temporal and\nspatial consistency, preserving intricate details, and incorporating user\nguidance effectively. To overcome these challenges, we introduce Splat4D, a\nnovel framework enabling high-fidelity 4D content generation from a monocular\nvideo. Splat4D achieves superior performance while maintaining faithful\nspatial-temporal coherence by leveraging multi-view rendering, inconsistency\nidentification, a video diffusion model, and an asymmetric U-Net for\nrefinement. Through extensive evaluations on public benchmarks, Splat4D\nconsistently demonstrates state-of-the-art performance across various metrics,\nunderscoring the efficacy of our approach. Additionally, the versatility of\nSplat4D is validated in various applications such as text/image conditioned 4D\ngeneration, 4D human generation, and text-guided content editing, producing\ncoherent outcomes following user instructions.",
        "url": "http://arxiv.org/abs/2508.07557v1",
        "published_date": "2025-08-11T02:35:53+00:00",
        "updated_date": "2025-08-11T02:35:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minghao Yin",
            "Yukang Cao",
            "Songyou Peng",
            "Kai Han"
        ],
        "tldr": "Splat4D is a novel framework for generating high-fidelity, temporally and spatially consistent 4D content from monocular videos using a video diffusion model and Gaussian splatting.",
        "tldr_zh": "Splat4D是一个新颖的框架，它使用视频扩散模型和高斯溅射，从单眼视频生成高保真、时间空间一致的4D内容。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cut2Next: Generating Next Shot via In-Context Tuning",
        "summary": "Effective multi-shot generation demands purposeful, film-like transitions and\nstrict cinematic continuity. Current methods, however, often prioritize basic\nvisual consistency, neglecting crucial editing patterns (e.g., shot/reverse\nshot, cutaways) that drive narrative flow for compelling storytelling. This\nyields outputs that may be visually coherent but lack narrative sophistication\nand true cinematic integrity. To bridge this, we introduce Next Shot Generation\n(NSG): synthesizing a subsequent, high-quality shot that critically conforms to\nprofessional editing patterns while upholding rigorous cinematic continuity.\nOur framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs\nin-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This\nstrategy uses Relational Prompts to define overall context and inter-shot\nediting styles. Individual Prompts then specify per-shot content and\ncinematographic attributes. Together, these guide Cut2Next to generate\ncinematically appropriate next shots. Architectural innovations, Context-Aware\nCondition Injection (CACI) and Hierarchical Attention Mask (HAM), further\nintegrate these diverse signals without introducing new parameters. We\nconstruct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with\nhierarchical prompts, and introduce CutBench for evaluation. Experiments show\nCut2Next excels in visual consistency and text fidelity. Crucially, user\nstudies reveal a strong preference for Cut2Next, particularly for its adherence\nto intended editing patterns and overall cinematic continuity, validating its\nability to generate high-quality, narratively expressive, and cinematically\ncoherent subsequent shots.",
        "url": "http://arxiv.org/abs/2508.08244v1",
        "published_date": "2025-08-11T17:56:59+00:00",
        "updated_date": "2025-08-11T17:56:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jingwen He",
            "Hongbo Liu",
            "Jiajun Li",
            "Ziqi Huang",
            "Yu Qiao",
            "Wanli Ouyang",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces Cut2Next, a Diffusion Transformer-based framework for generating subsequent shots in a video that conform to professional editing patterns and maintain cinematic continuity, using in-context tuning and hierarchical prompting.",
        "tldr_zh": "该论文介绍了一种名为Cut2Next的框架，该框架基于扩散Transformer，利用上下文调整和分层提示来生成视频中的后续镜头，使其符合专业的编辑模式并保持电影的连贯性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix",
        "summary": "While video generation models excel at producing high-quality monocular\nvideos, generating 3D stereoscopic and spatial videos for immersive\napplications remains an underexplored challenge. We present a pose-free and\ntraining-free method that leverages an off-the-shelf monocular video generation\nmodel to produce immersive 3D videos. Our approach first warps the generated\nmonocular video into pre-defined camera viewpoints using estimated depth\ninformation, then applies a novel \\textit{frame matrix} inpainting framework.\nThis framework utilizes the original video generation model to synthesize\nmissing content across different viewpoints and timestamps, ensuring spatial\nand temporal consistency without requiring additional model fine-tuning.\nMoreover, we develop a \\dualupdate~scheme that further improves the quality of\nvideo inpainting by alleviating the negative effects propagated from\ndisoccluded areas in the latent space. The resulting multi-view videos are then\nadapted into stereoscopic pairs or optimized into 4D Gaussians for spatial\nvideo synthesis. We validate the efficacy of our proposed method by conducting\nexperiments on videos from various generative models, such as Sora, Lumiere,\nWALT, and Zeroscope. The experiments demonstrate that our method has a\nsignificant improvement over previous methods. Project page at:\nhttps://daipengwa.github.io/S-2VG_ProjectPage/",
        "url": "http://arxiv.org/abs/2508.08048v1",
        "published_date": "2025-08-11T14:50:03+00:00",
        "updated_date": "2025-08-11T14:50:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peng Dai",
            "Feitong Tan",
            "Qiangeng Xu",
            "Yihua Huang",
            "David Futschik",
            "Ruofei Du",
            "Sean Fanello",
            "Yinda Zhang",
            "Xiaojuan Qi"
        ],
        "tldr": "The paper introduces S^2VG, a training-free method to generate 3D stereoscopic and spatial videos from monocular video generation models by warping, frame matrix inpainting, and a dual update scheme to improve consistency and quality.",
        "tldr_zh": "该论文介绍了 S^2VG，一种无需训练的方法，通过变形、帧矩阵修复和双重更新方案，从单目视频生成模型生成 3D 立体和空间视频，从而提高一致性和质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "LaVieID: Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation",
        "summary": "In this paper, we present LaVieID, a novel \\underline{l}ocal\n\\underline{a}utoregressive \\underline{vi}d\\underline{e}o diffusion framework\ndesigned to tackle the challenging \\underline{id}entity-preserving\ntext-to-video task. The key idea of LaVieID is to mitigate the loss of identity\ninformation inherent in the stochastic global generation process of diffusion\ntransformers (DiTs) from both spatial and temporal perspectives. Specifically,\nunlike the global and unstructured modeling of facial latent states in existing\nDiTs, LaVieID introduces a local router to explicitly represent latent states\nby weighted combinations of fine-grained local facial structures. This\nalleviates undesirable feature interference and encourages DiTs to capture\ndistinctive facial characteristics. Furthermore, a temporal autoregressive\nmodule is integrated into LaVieID to refine denoised latent tokens before video\ndecoding. This module divides latent tokens temporally into chunks, exploiting\ntheir long-range temporal dependencies to predict biases for rectifying tokens,\nthereby significantly enhancing inter-frame identity consistency. Consequently,\nLaVieID can generate high-fidelity personalized videos and achieve\nstate-of-the-art performance. Our code and models are available at\nhttps://github.com/ssugarwh/LaVieID.",
        "url": "http://arxiv.org/abs/2508.07603v1",
        "published_date": "2025-08-11T04:13:32+00:00",
        "updated_date": "2025-08-11T04:13:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenhui Song",
            "Hanhui Li",
            "Jiehui Huang",
            "Panwen Hu",
            "Yuhao Cheng",
            "Long Chen",
            "Yiqiang Yan",
            "Xiaodan Liang"
        ],
        "tldr": "LaVieID is a novel diffusion transformer framework that leverages local representations and temporal autoregression to improve identity preservation in text-to-video generation, achieving state-of-the-art performance.",
        "tldr_zh": "LaVieID 是一种新颖的扩散Transformer框架，它利用局部表示和时间自回归来提高文本到视频生成中的身份保持能力，并实现了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]