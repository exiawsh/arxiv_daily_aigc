[
    {
        "title": "Yan: Foundational Interactive Video Generation",
        "summary": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.",
        "url": "http://arxiv.org/abs/2508.08601v1",
        "published_date": "2025-08-12T03:34:21+00:00",
        "updated_date": "2025-08-12T03:34:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yan Team"
        ],
        "tldr": "The paper introduces Yan, a comprehensive framework for interactive video generation, encompassing simulation, generation, and editing through three core modules: AAA-level simulation, multi-modal generation, and multi-granularity editing.",
        "tldr_zh": "该论文介绍了Yan，一个用于交互式视频生成的综合框架，通过三个核心模块：AAA级模拟、多模态生成和多粒度编辑，涵盖了模拟、生成和编辑。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space",
        "summary": "Generating human videos with realistic and controllable motions is a\nchallenging task. While existing methods can generate visually compelling\nvideos, they lack separate control over four key video elements: foreground\nsubject, background video, human trajectory and action patterns. In this paper,\nwe propose a decomposed human motion control and video generation framework\nthat explicitly decouples motion from appearance, subject from background, and\naction from trajectory, enabling flexible mix-and-match composition of these\nelements. Concretely, we first build a ground-aware 3D world coordinate system\nand perform motion editing directly in the 3D space. Trajectory control is\nimplemented by unprojecting edited 2D trajectories into 3D with focal-length\ncalibration and coordinate transformation, followed by speed alignment and\norientation adjustment; actions are supplied by a motion bank or generated via\ntext-to-motion methods. Then, based on modern text-to-video diffusion\ntransformer models, we inject the subject as tokens for full attention,\nconcatenate the background along the channel dimension, and add motion\n(trajectory and action) control signals by addition. Such a design opens up the\npossibility for us to generate realistic videos of anyone doing anything\nanywhere. Extensive experiments on benchmark datasets and real-world cases\ndemonstrate that our method achieves state-of-the-art performance on both\nelement-wise controllability and overall video quality.",
        "url": "http://arxiv.org/abs/2508.08588v1",
        "published_date": "2025-08-12T03:02:23+00:00",
        "updated_date": "2025-08-12T03:02:23+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Jingyun Liang",
            "Jingkai Zhou",
            "Shikai Li",
            "Chenjie Cao",
            "Lei Sun",
            "Yichen Qian",
            "Weihua Chen",
            "Fan Wang"
        ],
        "tldr": "The paper proposes a decomposed video generation framework, RealisMotion, that allows for separate control over foreground subject, background video, human trajectory, and action patterns by operating in a ground-aware 3D world space.",
        "tldr_zh": "该论文提出了一个分解的视频生成框架RealisMotion，通过在感知地面的3D世界空间中操作，可以分别控制前景主体、背景视频、人体轨迹和动作模式。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling",
        "summary": "Despite recent advances, long-sequence video generation frameworks still\nsuffer from significant limitations: poor assistive capability, suboptimal\nvisual quality, and limited expressiveness. To mitigate these limitations, we\npropose MAViS, an end-to-end multi-agent collaborative framework for\nlong-sequence video storytelling. MAViS orchestrates specialized agents across\nmultiple stages, including script writing, shot designing, character modeling,\nkeyframe generation, video animation, and audio generation. In each stage,\nagents operate under the 3E Principle -- Explore, Examine, and Enhance -- to\nensure the completeness of intermediate outputs. Considering the capability\nlimitations of current generative models, we propose the Script Writing\nGuidelines to optimize compatibility between scripts and generative tools.\nExperimental results demonstrate that MAViS achieves state-of-the-art\nperformance in assistive capability, visual quality, and video expressiveness.\nIts modular framework further enables scalability with diverse generative\nmodels and tools. With just a brief user prompt, MAViS is capable of producing\nhigh-quality, expressive long-sequence video storytelling, enriching\ninspirations and creativity for users. To the best of our knowledge, MAViS is\nthe only framework that provides multimodal design output -- videos with\nnarratives and background music.",
        "url": "http://arxiv.org/abs/2508.08487v1",
        "published_date": "2025-08-11T21:42:41+00:00",
        "updated_date": "2025-08-11T21:42:41+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MA"
        ],
        "authors": [
            "Qian Wang",
            "Ziqi Huang",
            "Ruoxi Jia",
            "Paul Debevec",
            "Ning Yu"
        ],
        "tldr": "The paper introduces MAViS, a multi-agent framework for generating long-sequence videos with improved assistive capability, visual quality, and expressiveness through a collaborative, modular approach.",
        "tldr_zh": "该论文介绍了一种名为 MAViS 的多智能体框架，用于生成长序列视频，通过协作和模块化的方法，提高了辅助能力、视觉质量和表现力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]