[
    {
        "title": "Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models",
        "summary": "Recent advances in generative models have sparked exciting new possibilities\nin the field of autonomous vehicles. Specifically, video generation models are\nnow being explored as controllable virtual testing environments.\nSimultaneously, end-to-end (E2E) driving models have emerged as a streamlined\nalternative to conventional modular autonomous driving systems, gaining\npopularity for their simplicity and scalability. However, the application of\nthese techniques to simulation and planning raises important questions. First,\nwhile video generation models can generate increasingly realistic videos, can\nthese videos faithfully adhere to the specified conditions and be realistic\nenough for E2E autonomous planner evaluation? Second, given that data is\ncrucial for understanding and controlling E2E planners, how can we gain deeper\ninsights into their biases and improve their ability to generalize to\nout-of-distribution scenarios? In this work, we bridge the gap between the\ndriving models and generative world models (Drive&Gen) to address these\nquestions. We propose novel statistical measures leveraging E2E drivers to\nevaluate the realism of generated videos. By exploiting the controllability of\nthe video generation model, we conduct targeted experiments to investigate\ndistribution gaps affecting E2E planner performance. Finally, we show that\nsynthetic data produced by the video generation model offers a cost-effective\nalternative to real-world data collection. This synthetic data effectively\nimproves E2E model generalization beyond existing Operational Design Domains,\nfacilitating the expansion of autonomous vehicle services into new operational\ncontexts.",
        "url": "http://arxiv.org/abs/2510.06209v1",
        "published_date": "2025-10-07T17:58:32+00:00",
        "updated_date": "2025-10-07T17:58:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Wang",
            "Zhenpei Yang",
            "Yijing Bai",
            "Yingwei Li",
            "Yuliang Zou",
            "Bo Sun",
            "Abhijit Kundu",
            "Jose Lezama",
            "Luna Yue Huang",
            "Zehao Zhu",
            "Jyh-Jing Hwang",
            "Dragomir Anguelov",
            "Mingxing Tan",
            "Chiyu Max Jiang"
        ],
        "tldr": "This paper introduces a framework (Drive&Gen) to co-evaluate end-to-end driving models and video generation models, using driving models to assess the realism of generated videos and then using the synthetic data to improve the driving models' generalization.",
        "tldr_zh": "本文介绍了一个框架(Drive&Gen)，用于共同评估端到端驾驶模型和视频生成模型。该框架利用驾驶模型评估生成视频的真实性，并利用合成数据来提高驾驶模型的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Controllable Audio-Visual Viewpoint Generation from 360° Spatial Information",
        "summary": "The generation of sounding videos has seen significant advancements with the\nadvent of diffusion models. However, existing methods often lack the\nfine-grained control needed to generate viewpoint-specific content from larger,\nimmersive 360-degree environments. This limitation restricts the creation of\naudio-visual experiences that are aware of off-camera events. To the best of\nour knowledge, this is the first work to introduce a framework for controllable\naudio-visual generation, addressing this unexplored gap. Specifically, we\npropose a diffusion model by introducing a set of powerful conditioning signals\nderived from the full 360-degree space: a panoramic saliency map to identify\nregions of interest, a bounding-box-aware signed distance map to define the\ntarget viewpoint, and a descriptive caption of the entire scene. By integrating\nthese controls, our model generates spatially-aware viewpoint videos and audios\nthat are coherently influenced by the broader, unseen environmental context,\nintroducing a strong controllability that is essential for realistic and\nimmersive audio-visual generation. We show audiovisual examples proving the\neffectiveness of our framework.",
        "url": "http://arxiv.org/abs/2510.06060v1",
        "published_date": "2025-10-07T15:53:31+00:00",
        "updated_date": "2025-10-07T15:53:31+00:00",
        "categories": [
            "cs.MM",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Christian Marinoni",
            "Riccardo Fosco Gramaccioni",
            "Eleonora Grassucci",
            "Danilo Comminiello"
        ],
        "tldr": "This paper introduces a diffusion model framework for controllable audio-visual viewpoint generation from 360° spatial information, addressing the limitations of existing methods in creating viewpoint-specific content from immersive environments using panoramic saliency maps, bounding-box-aware signed distance maps, and scene captions as conditioning signals.",
        "tldr_zh": "本文介绍了一种基于扩散模型的框架，用于从360°空间信息中生成可控的视听视角。该框架利用全景显著图、边界框感知的符号距离图和场景描述作为条件信号，解决了现有方法在从沉浸式环境中创建特定视角内容方面的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]