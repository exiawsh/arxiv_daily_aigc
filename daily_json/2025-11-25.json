[
    {
        "title": "Growing with the Generator: Self-paced GRPO for Video Generation",
        "summary": "Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-aware GRPO framework in which reward feedback co-evolves with the generator. Our method introduces a progressive reward mechanism that automatically shifts its emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases. This self-paced curriculum alleviates reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization. Experiments on VBench across multiple video generation backbones demonstrate consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards, validating the effectiveness and generality of Self-Paced GRPO.",
        "url": "http://arxiv.org/abs/2511.19356v1",
        "published_date": "2025-11-24T17:56:03+00:00",
        "updated_date": "2025-11-24T17:56:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rui Li",
            "Yuanzhi Liang",
            "Ziqi Ni",
            "Haibing Huang",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "tldr": "This paper introduces Self-Paced GRPO, a reinforcement learning framework for video generation where the reward model adapts alongside the generator, leading to more stable and effective optimization and improved video quality and semantic alignment.",
        "tldr_zh": "本文介绍了 Self-Paced GRPO，一种用于视频生成的强化学习框架，其中奖励模型与生成器同步适应，从而实现更稳定有效的优化，并提高视频质量和语义对齐。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis",
        "summary": "Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.",
        "url": "http://arxiv.org/abs/2511.19319v1",
        "published_date": "2025-11-24T17:14:19+00:00",
        "updated_date": "2025-11-24T17:14:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lingwei Dang",
            "Zonghan Li",
            "Juntong Li",
            "Hongwen Zhang",
            "Liang An",
            "Yebin Liu",
            "Qingyao Wu"
        ],
        "tldr": "This paper introduces SyncMV4D, a novel diffusion-based approach for generating synchronized multi-view hand-object interaction (HOI) videos and 4D motions, addressing limitations of single-view methods and reliance on controlled lab settings in existing 3D HOI approaches.",
        "tldr_zh": "本文介绍了 SyncMV4D，一种新颖的基于扩散的方法，用于生成同步的多视角手-物体交互 (HOI) 视频和 4D 运动，解决了单视角方法的局限性以及现有 3D HOI 方法对受控实验室环境的依赖。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning Plug-and-play Memory for Guiding Video Diffusion Models",
        "summary": "Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.",
        "url": "http://arxiv.org/abs/2511.19229v1",
        "published_date": "2025-11-24T15:42:23+00:00",
        "updated_date": "2025-11-24T15:42:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Selena Song",
            "Ziming Xu",
            "Zijun Zhang",
            "Kun Zhou",
            "Jiaxian Guo",
            "Lianhui Qin",
            "Biwei Huang"
        ],
        "tldr": "The paper introduces DiT-Mem, a plug-and-play memory module for Diffusion Transformer video generation models that enhances physical rule following and video fidelity by injecting world knowledge through a learnable memory encoder.",
        "tldr_zh": "该论文介绍了一种名为DiT-Mem的即插即用记忆模块，用于扩散Transformer视频生成模型。通过可学习的记忆编码器注入世界知识，从而增强物理规则的遵循和视频的逼真度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation",
        "summary": "Direct Preference Optimization (DPO) has shown promising results in aligning generative outputs with human preferences by distinguishing between chosen and rejected samples. However, a critical limitation of DPO is likelihood displacement, where the probabilities of chosen samples paradoxically decrease during training, undermining the quality of generation. Although this issue has been investigated in autoregressive models, its impact within diffusion-based models remains largely unexplored. This gap leads to suboptimal performance in tasks involving video generation. To address this, we conduct a formal analysis of DPO loss through updating policy within the diffusion framework, which describes how the updating of specific training samples influences the model's predictions on other samples. Using this tool, we identify two main failure modes: (1) Optimization Conflict, which arises from small reward margins between chosen and rejected samples, and (2) Suboptimal Maximization, caused by large reward margins. Informed by these insights, we introduce a novel solution named Policy-Guided DPO (PG-DPO), combining Adaptive Rejection Scaling (ARS) and Implicit Preference Regularization (IPR) to effectively mitigate likelihood displacement. Experiments show that PG-DPO outperforms existing methods in both quantitative metrics and qualitative evaluations, offering a robust solution for improving preference alignment in video generation tasks.",
        "url": "http://arxiv.org/abs/2511.19049v1",
        "published_date": "2025-11-24T12:37:49+00:00",
        "updated_date": "2025-11-24T12:37:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruojun Xu",
            "Yu Kai",
            "Xuhua Ren",
            "Jiaxiang Cheng",
            "Bing Ma",
            "Tianxiang Zheng",
            "Qinhlin Lu"
        ],
        "tldr": "This paper addresses the likelihood displacement issue in DPO-based diffusion models for video generation, proposing a Policy-Guided DPO (PG-DPO) method that outperforms existing approaches.",
        "tldr_zh": "本文解决了基于DPO的扩散模型在视频生成中的可能性位移问题，提出了一种策略引导的DPO（PG-DPO）方法，该方法优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation",
        "summary": "Video generation models have made significant progress in generating realistic content, enabling applications in simulation, gaming, and film making. However, current generated videos still contain visual artifacts arising from 3D inconsistencies, e.g., objects and structures deforming under changes in camera pose, which can undermine user experience and simulation fidelity. Motivated by recent findings on representation alignment for diffusion models, we hypothesize that improving the multi-view consistency of video diffusion representations will yield more 3D-consistent video generation. Through detailed analysis on multiple recent camera-controlled video diffusion models we reveal strong correlations between 3D-consistent representations and videos. We also propose ViCoDR, a new approach for improving the 3D consistency of video models by learning multi-view consistent diffusion representations. We evaluate ViCoDR on camera controlled image-to-video, text-to-video, and multi-view generation models, demonstrating significant improvements in the 3D consistency of the generated videos. Project page: https://danier97.github.io/ViCoDR.",
        "url": "http://arxiv.org/abs/2511.18991v1",
        "published_date": "2025-11-24T11:16:55+00:00",
        "updated_date": "2025-11-24T11:16:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Duolikun Danier",
            "Ge Gao",
            "Steven McDonagh",
            "Changjian Li",
            "Hakan Bilen",
            "Oisin Mac Aodha"
        ],
        "tldr": "This paper proposes ViCoDR, a method to improve 3D consistency in video generation by learning multi-view consistent diffusion representations, demonstrating significant improvements across various camera-controlled video generation tasks.",
        "tldr_zh": "本文提出了一种名为ViCoDR 的方法，通过学习多视角一致的扩散表示来提高视频生成中的 3D 一致性，并在各种相机控制的视频生成任务中展示了显著的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MagicWorld: Interactive Geometry-driven Video World Exploration",
        "summary": "Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.",
        "url": "http://arxiv.org/abs/2511.18886v1",
        "published_date": "2025-11-24T08:41:28+00:00",
        "updated_date": "2025-11-24T08:41:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guangyuan Li",
            "Siming Zheng",
            "Shuolin Xu",
            "Jinwei Chen",
            "Bo Li",
            "Xiaobin Hu",
            "Lei Zhao",
            "Peng-Tao Jiang"
        ],
        "tldr": "MagicWorld introduces geometric priors and historical retrieval to improve the stability and continuity of interactive video world models, addressing limitations in existing instruction-driven scene evolution methods.",
        "tldr_zh": "MagicWorld引入了几何先验和历史检索，以提高交互式视频世界模型的稳定性和连续性，解决了现有指令驱动场景演化方法的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HunyuanVideo 1.5 Technical Report",
        "summary": "We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.",
        "url": "http://arxiv.org/abs/2511.18870v1",
        "published_date": "2025-11-24T08:22:07+00:00",
        "updated_date": "2025-11-24T08:22:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bing Wu",
            "Chang Zou",
            "Changlin Li",
            "Duojun Huang",
            "Fang Yang",
            "Hao Tan",
            "Jack Peng",
            "Jianbing Wu",
            "Jiangfeng Xiong",
            "Jie Jiang",
            "Linus",
            "Patrol",
            "Peizhen Zhang",
            "Peng Chen",
            "Penghao Zhao",
            "Qi Tian",
            "Songtao Liu",
            "Weijie Kong",
            "Weiyan Wang",
            "Xiao He",
            "Xin Li",
            "Xinchi Deng",
            "Xuefei Zhe",
            "Yang Li",
            "Yanxin Long",
            "Yuanbo Peng",
            "Yue Wu",
            "Yuhong Liu",
            "Zhenyu Wang",
            "Zuozhuo Dai",
            "Bo Peng",
            "Coopers Li",
            "Gu Gong",
            "Guojian Xiao",
            "Jiahe Tian",
            "Jiaxin Lin",
            "Jie Liu",
            "Jihong Zhang",
            "Jiesong Lian",
            "Kaihang Pan",
            "Lei Wang",
            "Lin Niu",
            "Mingtao Chen",
            "Mingyang Chen",
            "Mingzhe Zheng",
            "Miles Yang",
            "Qiangqiang Hu",
            "Qi Yang",
            "Qiuyong Xiao",
            "Runzhou Wu",
            "Ryan Xu",
            "Rui Yuan",
            "Shanshan Sang",
            "Shisheng Huang",
            "Siruis Gong",
            "Shuo Huang",
            "Weiting Guo",
            "Xiang Yuan",
            "Xiaojia Chen",
            "Xiawei Hu",
            "Wenzhi Sun",
            "Xiele Wu",
            "Xianshun Ren",
            "Xiaoyan Yuan",
            "Xiaoyue Mi",
            "Yepeng Zhang",
            "Yifu Sun",
            "Yiting Lu",
            "Yitong Li",
            "You Huang",
            "Yu Tang",
            "Yixuan Li",
            "Yuhang Deng",
            "Yuan Zhou",
            "Zhichao Hu",
            "Zhiguang Liu",
            "Zhihe Yang",
            "Zilin Yang",
            "Zhenzhi Lu",
            "Zixiang Zhou",
            "Zhao Zhong"
        ],
        "tldr": "HunyuanVideo 1.5 is a lightweight (8.3B parameters) open-source video generation model claiming state-of-the-art visual quality and motion coherence, with efficient inference on consumer GPUs, utilizing a DiT architecture and various training techniques.",
        "tldr_zh": "HunyuanVideo 1.5是一个轻量级（83亿参数）的开源视频生成模型，声称具有最先进的视觉质量和运动连贯性，并且可以在消费级GPU上进行高效推理，它利用了DiT架构和各种训练技术。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]