[
    {
        "title": "Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence",
        "summary": "The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.",
        "url": "http://arxiv.org/abs/2512.03905v1",
        "published_date": "2025-12-03T15:51:11+00:00",
        "updated_date": "2025-12-03T15:51:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuai Yang",
            "Junxin Lin",
            "Yifan Zhou",
            "Ziwei Liu",
            "Chen Change Loy"
        ],
        "tldr": "This paper introduces FRESCO, a zero-shot video translation and editing method that improves temporal consistency by integrating intra- and inter-frame correspondence for more robust spatial-temporal constraints, demonstrating superior performance in video-to-video translation and text-guided video editing.",
        "tldr_zh": "该论文介绍了FRESCO，一种零样本视频翻译和编辑方法，通过整合帧内和帧间对应关系来构建更强大的时空约束，从而提高时间一致性，并在视频到视频翻译和文本引导的视频编辑中展示了卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation",
        "summary": "We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.",
        "url": "http://arxiv.org/abs/2512.03621v1",
        "published_date": "2025-12-03T09:55:25+00:00",
        "updated_date": "2025-12-03T09:55:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaokun Li",
            "Shuaixian Wang",
            "Mantang Guo",
            "Jiehui Huang",
            "Taojun Ding",
            "Mu Hu",
            "Kaixuan Wang",
            "Shaojie Shen",
            "Guang Tan"
        ],
        "tldr": "ReCamDriving is a novel framework for camera-controlled video generation from monocular videos using 3D Gaussian Splatting (3DGS) for geometric guidance, addressing limitations of repair-based and LiDAR-based methods. They introduce a two-stage training approach and a cross-trajectory data curation strategy, along with the ParaDrive dataset.",
        "tldr_zh": "ReCamDriving是一个新的框架，利用3D高斯溅射（3DGS）进行几何引导，从单目视频生成相机控制的视频，解决了基于修复和基于激光雷达方法的局限性。他们引入了两阶段训练方法和交叉轨迹数据管理策略，并提出了ParaDrive数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LAMP: Language-Assisted Motion Planning for Controllable Video Generation",
        "summary": "Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications.",
        "url": "http://arxiv.org/abs/2512.03619v1",
        "published_date": "2025-12-03T09:51:13+00:00",
        "updated_date": "2025-12-03T09:51:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muhammed Burak Kizil",
            "Enes Sanli",
            "Niloy J. Mitra",
            "Erkut Erdem",
            "Aykut Erdem",
            "Duygu Ceylan"
        ],
        "tldr": "LAMP uses LLMs to translate natural language into 3D trajectories for object and camera motion in video generation, offering improved controllability and alignment with user intent.",
        "tldr_zh": "LAMP利用大型语言模型（LLM）将自然语言转化为视频生成中物体和相机运动的3D轨迹，从而提高可控性并与用户意图对齐。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GeoVideo: Introducing Geometric Regularization into Video Generation Model",
        "summary": "Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.",
        "url": "http://arxiv.org/abs/2512.03453v1",
        "published_date": "2025-12-03T05:11:57+00:00",
        "updated_date": "2025-12-03T05:11:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yunpeng Bai",
            "Shaoheng Fang",
            "Chaohui Yu",
            "Fan Wang",
            "Qixing Huang"
        ],
        "tldr": "This paper introduces geometric regularization, specifically depth prediction with a multi-view geometric loss, into video generation models to improve spatio-temporal coherence and shape consistency.",
        "tldr_zh": "本文通过将几何正则化（特别是深度预测和多视角几何损失）引入视频生成模型，以提高时空一致性和形状一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers",
        "summary": "Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications.\n  We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve $1.87\\times$ and $2.37\\times$ speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).",
        "url": "http://arxiv.org/abs/2512.03451v1",
        "published_date": "2025-12-03T05:08:18+00:00",
        "updated_date": "2025-12-03T05:08:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Zhiye Song",
            "Steve Dai",
            "Ben Keller",
            "Brucek Khailany"
        ],
        "tldr": "GalaxyDiT introduces a training-free method to accelerate video generation in Diffusion Transformers by identifying and reusing optimal proxies, achieving significant speedups with minimal quality loss.",
        "tldr_zh": "GalaxyDiT 提出了一种无需训练的方法，通过识别和重用最佳代理来加速 Diffusion Transformers 中的视频生成，从而在保证质量损失最小化的同时实现显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation",
        "summary": "Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\\to$4D$\\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.",
        "url": "http://arxiv.org/abs/2512.03350v1",
        "published_date": "2025-12-03T01:30:45+00:00",
        "updated_date": "2025-12-03T01:30:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Yuan",
            "Tharindu Wickremasinghe",
            "Zeeshan Nadir",
            "Xijun Wang",
            "Yiheng Chi",
            "Stanley H. Chan"
        ],
        "tldr": "SeeU proposes a 2D->4D->2D framework to learn continuous 4D dynamics from sparse 2D videos, enabling novel view and temporal generation with physical consistency.",
        "tldr_zh": "SeeU 提出了一个 2D->4D->2D 的框架，从稀疏的 2D 视频中学习连续的 4D 动力学，从而实现具有物理一致性的新视角和时间生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MultiShotMaster: A Controllable Multi-Shot Video Generation Framework",
        "summary": "Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.",
        "url": "http://arxiv.org/abs/2512.03041v1",
        "published_date": "2025-12-02T18:59:48+00:00",
        "updated_date": "2025-12-02T18:59:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qinghe Wang",
            "Xiaoyu Shi",
            "Baolu Li",
            "Weikang Bian",
            "Quande Liu",
            "Huchuan Lu",
            "Xintao Wang",
            "Pengfei Wan",
            "Kun Gai",
            "Xu Jia"
        ],
        "tldr": "The paper introduces MultiShotMaster, a framework for generating controllable multi-shot videos by extending a single-shot model with novel RoPE variants for narrative coherence, grounding, and flexible shot arrangement, also addresses data scarcity through an automated annotation pipeline.",
        "tldr_zh": "该论文介绍了MultiShotMaster，一个用于生成可控的多镜头视频的框架。它通过使用新型RoPE变体扩展单镜头模型，实现了叙事连贯性、对齐和灵活的镜头安排。同时，还通过自动标注流程解决了数据稀缺问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation",
        "summary": "We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.",
        "url": "http://arxiv.org/abs/2512.03040v1",
        "published_date": "2025-12-02T18:59:44+00:00",
        "updated_date": "2025-12-02T18:59:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zeqi Xiao",
            "Yiwei Zhao",
            "Lingxiao Li",
            "Yushi Lan",
            "Yu Ning",
            "Rahul Garg",
            "Roshni Cooper",
            "Mohammad H. Taghavi",
            "Xingang Pan"
        ],
        "tldr": "The paper introduces Video4Spatial, a video diffusion framework that uses video-based scene context to perform complex spatial tasks like scene navigation and object grounding, demonstrating visuospatial reasoning capabilities without relying on auxiliary modalities.",
        "tldr_zh": "该论文介绍了Video4Spatial，一个视频扩散框架，它使用基于视频的场景上下文来执行复杂的空间任务，如场景导航和物体定位，展示了视觉空间推理能力，且不依赖于辅助模态。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]