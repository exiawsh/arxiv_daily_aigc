[
    {
        "title": "EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning",
        "summary": "Recent advances in foundation models highlight a clear trend toward\nunification and scaling, showing emergent capabilities across diverse domains.\nWhile image generation and editing have rapidly transitioned from task-specific\nto unified frameworks, video generation and editing remain fragmented due to\narchitectural limitations and data scarcity. In this work, we introduce\nEditVerse, a unified framework for image and video generation and editing\nwithin a single model. By representing all modalities, i.e., text, image, and\nvideo, as a unified token sequence, EditVerse leverages self-attention to\nachieve robust in-context learning, natural cross-modal knowledge transfer, and\nflexible handling of inputs and outputs with arbitrary resolutions and\ndurations. To address the lack of video editing training data, we design a\nscalable data pipeline that curates 232K video editing samples and combines\nthem with large-scale image and video datasets for joint training. Furthermore,\nwe present EditVerseBench, the first benchmark for instruction-based video\nediting covering diverse tasks and resolutions. Extensive experiments and user\nstudies demonstrate that EditVerse achieves state-of-the-art performance,\nsurpassing existing open-source and commercial models, while exhibiting\nemergent editing and generation abilities across modalities.",
        "url": "http://arxiv.org/abs/2509.20360v1",
        "published_date": "2025-09-24T17:59:30+00:00",
        "updated_date": "2025-09-24T17:59:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuan Ju",
            "Tianyu Wang",
            "Yuqian Zhou",
            "He Zhang",
            "Qing Liu",
            "Nanxuan Zhao",
            "Zhifei Zhang",
            "Yijun Li",
            "Yuanhao Cai",
            "Shaoteng Liu",
            "Daniil Pakhomov",
            "Zhe Lin",
            "Soo Ye Kim",
            "Qiang Xu"
        ],
        "tldr": "EditVerse is a unified framework for image and video generation and editing, leveraging in-context learning and a large-scale dataset to achieve state-of-the-art performance across modalities.",
        "tldr_zh": "EditVerse是一个统一的图像和视频生成与编辑框架，它利用上下文学习和大规模数据集，在多种模态上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation",
        "summary": "Existing video generation models excel at producing photo-realistic videos\nfrom text or images, but often lack physical plausibility and 3D\ncontrollability. To overcome these limitations, we introduce PhysCtrl, a novel\nframework for physics-grounded image-to-video generation with physical\nparameters and force control. At its core is a generative physics network that\nlearns the distribution of physical dynamics across four materials (elastic,\nsand, plasticine, and rigid) via a diffusion model conditioned on physics\nparameters and applied forces. We represent physical dynamics as 3D point\ntrajectories and train on a large-scale synthetic dataset of 550K animations\ngenerated by physics simulators. We enhance the diffusion model with a novel\nspatiotemporal attention block that emulates particle interactions and\nincorporates physics-based constraints during training to enforce physical\nplausibility. Experiments show that PhysCtrl generates realistic,\nphysics-grounded motion trajectories which, when used to drive image-to-video\nmodels, yield high-fidelity, controllable videos that outperform existing\nmethods in both visual quality and physical plausibility. Project Page:\nhttps://cwchenwang.github.io/physctrl",
        "url": "http://arxiv.org/abs/2509.20358v1",
        "published_date": "2025-09-24T17:58:04+00:00",
        "updated_date": "2025-09-24T17:58:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Wang",
            "Chuhao Chen",
            "Yiming Huang",
            "Zhiyang Dou",
            "Yuan Liu",
            "Jiatao Gu",
            "Lingjie Liu"
        ],
        "tldr": "PhysCtrl is a novel framework for controllable and physics-grounded video generation using a diffusion model trained on a large dataset of simulated physical dynamics, outperforming existing methods in visual quality and physical plausibility.",
        "tldr_zh": "PhysCtrl是一个新颖的框架，用于可控的、基于物理的视频生成，它使用在大型模拟物理动态数据集上训练的扩散模型，在视觉质量和物理合理性方面优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "4D Driving Scene Generation With Stereo Forcing",
        "summary": "Current generative models struggle to synthesize dynamic 4D driving scenes\nthat simultaneously support temporal extrapolation and spatial novel view\nsynthesis (NVS) without per-scene optimization. Bridging generation and novel\nview synthesis remains a major challenge. We present PhiGenesis, a unified\nframework for 4D scene generation that extends video generation techniques with\ngeometric and temporal consistency. Given multi-view image sequences and camera\nparameters, PhiGenesis produces temporally continuous 4D Gaussian splatting\nrepresentations along target 3D trajectories. In its first stage, PhiGenesis\nleverages a pre-trained video VAE with a novel range-view adapter to enable\nfeed-forward 4D reconstruction from multi-view images. This architecture\nsupports single-frame or video inputs and outputs complete 4D scenes including\ngeometry, semantics, and motion. In the second stage, PhiGenesis introduces a\ngeometric-guided video diffusion model, using rendered historical 4D scenes as\npriors to generate future views conditioned on trajectories. To address\ngeometric exposure bias in novel views, we propose Stereo Forcing, a novel\nconditioning strategy that integrates geometric uncertainty during denoising.\nThis method enhances temporal coherence by dynamically adjusting generative\ninfluence based on uncertainty-aware perturbations. Our experimental results\ndemonstrate that our method achieves state-of-the-art performance in both\nappearance and geometric reconstruction, temporal generation and novel view\nsynthesis (NVS) tasks, while simultaneously delivering competitive performance\nin downstream evaluations. Homepage is at\n\\href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}.",
        "url": "http://arxiv.org/abs/2509.20251v1",
        "published_date": "2025-09-24T15:37:17+00:00",
        "updated_date": "2025-09-24T15:37:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Lu",
            "Zhuang Ma",
            "Guangfeng Jiang",
            "Wenhang Ge",
            "Bohan Li",
            "Yuzhan Cai",
            "Wenzhao Zheng",
            "Yunpeng Zhang",
            "Yingcong Chen"
        ],
        "tldr": "The paper introduces PhiGenesis, a framework for generating temporally consistent 4D driving scenes supporting both temporal extrapolation and novel view synthesis, using a video VAE and a geometric-guided video diffusion model with a novel 'Stereo Forcing' conditioning strategy.",
        "tldr_zh": "该论文介绍了一种名为 PhiGenesis 的框架，用于生成时间上一致的 4D 驾驶场景，支持时间外推和新视角合成。该框架使用视频 VAE 和几何引导的视频扩散模型，并采用了一种名为“立体强制”的新型条件策略。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion",
        "summary": "Recently, camera-controlled video generation has seen rapid development,\noffering more precise control over video generation. However, existing methods\npredominantly focus on camera control in perspective projection video\ngeneration, while geometrically consistent panoramic video generation remains\nchallenging. This limitation is primarily due to the inherent complexities in\npanoramic pose representation and spherical projection. To address this issue,\nwe propose CamPVG, the first diffusion-based framework for panoramic video\ngeneration guided by precise camera poses. We achieve camera position encoding\nfor panoramic images and cross-view feature aggregation based on spherical\nprojection. Specifically, we propose a panoramic Pl\\\"ucker embedding that\nencodes camera extrinsic parameters through spherical coordinate\ntransformation. This pose encoder effectively captures panoramic geometry,\novercoming the limitations of traditional methods when applied to\nequirectangular projections. Additionally, we introduce a spherical epipolar\nmodule that enforces geometric constraints through adaptive attention masking\nalong epipolar lines. This module enables fine-grained cross-view feature\naggregation, substantially enhancing the quality and consistency of generated\npanoramic videos. Extensive experiments demonstrate that our method generates\nhigh-quality panoramic videos consistent with camera trajectories, far\nsurpassing existing methods in panoramic video generation.",
        "url": "http://arxiv.org/abs/2509.19979v1",
        "published_date": "2025-09-24T10:34:24+00:00",
        "updated_date": "2025-09-24T10:34:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenhao Ji",
            "Chaohui Yu",
            "Junyao Gao",
            "Fan Wang",
            "Cairong Zhao"
        ],
        "tldr": "The paper introduces CamPVG, a diffusion-based framework for generating camera-controlled panoramic videos using a novel panoramic Plücker embedding and a spherical epipolar module for improved geometric consistency and quality.",
        "tldr_zh": "该论文介绍了 CamPVG，一个基于扩散的框架，用于生成相机控制的全景视频，它采用了一种新颖的全景 Plücker 嵌入和一个球形对极模块，以提高几何一致性和质量。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition",
        "summary": "Existing models often struggle with complex temporal changes, particularly\nwhen generating videos with gradual attribute transitions. The most common\nprompt interpolation approach for motion transitions often fails to handle\ngradual attribute transitions, where inconsistencies tend to become more\npronounced. In this work, we propose a simple yet effective method to extend\nexisting models for smooth and consistent attribute transitions, through\nintroducing frame-wise guidance during the denoising process. Our approach\nconstructs a data-specific transitional direction for each noisy latent,\nguiding the gradual shift from initial to final attributes frame by frame while\npreserving the motion dynamics of the video. Moreover, we present the\nControlled-Attribute-Transition Benchmark (CAT-Bench), which integrates both\nattribute and motion dynamics, to comprehensively evaluate the performance of\ndifferent models. We further propose two metrics to assess the accuracy and\nsmoothness of attribute transitions. Experimental results demonstrate that our\napproach performs favorably against existing baselines, achieving visual\nfidelity, maintaining alignment with text prompts, and delivering seamless\nattribute transitions. Code and CATBench are released:\nhttps://github.com/lynn-ling-lo/Prompt2Progression.",
        "url": "http://arxiv.org/abs/2509.19690v1",
        "published_date": "2025-09-24T01:58:22+00:00",
        "updated_date": "2025-09-24T01:58:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ling Lo",
            "Kelvin C. K. Chan",
            "Wen-Huang Cheng",
            "Ming-Hsuan Yang"
        ],
        "tldr": "This paper introduces a novel frame-wise guidance method for video diffusion models that enables seamless attribute transitions, along with a new benchmark (CAT-Bench) and metrics for evaluation. The experiments show improvement over existing methods.",
        "tldr_zh": "该论文提出了一种新的视频扩散模型逐帧引导方法，能够实现无缝的属性转换，同时还提出了一个新的基准测试 (CAT-Bench) 和评估指标。 实验表明，该方法优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]