[
    {
        "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model",
        "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.",
        "url": "http://arxiv.org/abs/2512.22096v1",
        "published_date": "2025-12-26T17:52:49+00:00",
        "updated_date": "2025-12-26T17:52:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaofeng Mao",
            "Zhen Li",
            "Chuanhao Li",
            "Xiaojie Xu",
            "Kaining Ying",
            "Tong He",
            "Jiangmiao Pang",
            "Yu Qiao",
            "Kaipeng Zhang"
        ],
        "tldr": "Yume-1.5 introduces a novel framework for generating interactive and continuous worlds from text or image prompts, addressing limitations of existing methods with its efficiency and text control.",
        "tldr_zh": "Yume-1.5 提出了一个新颖的框架，用于从文本或图像提示生成交互式和连续的世界，解决了现有方法的效率和文本控制的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars",
        "summary": "Real-time, streaming interactive avatars represent a critical yet challenging goal in digital human research. Although diffusion-based human avatar generation methods achieve remarkable success, their non-causal architecture and high computational costs make them unsuitable for streaming. Moreover, existing interactive approaches are typically limited to head-and-shoulder region, limiting their ability to produce gestures and body motions. To address these challenges, we propose a two-stage autoregressive adaptation and acceleration framework that applies autoregressive distillation and adversarial refinement to adapt a high-fidelity human video diffusion model for real-time, interactive streaming. To ensure long-term stability and consistency, we introduce three key components: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. Building on this framework, we develop a one-shot, interactive, human avatar model capable of generating both natural talking and listening behaviors with coherent gestures. Extensive experiments demonstrate that our method achieves state-of-the-art performance, surpassing existing approaches in generation quality, real-time efficiency, and interaction naturalness. Project page: https://streamavatar.github.io .",
        "url": "http://arxiv.org/abs/2512.22065v1",
        "published_date": "2025-12-26T15:41:24+00:00",
        "updated_date": "2025-12-26T15:41:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.HC"
        ],
        "authors": [
            "Zhiyao Sun",
            "Ziqiao Peng",
            "Yifeng Ma",
            "Yi Chen",
            "Zhengguang Zhou",
            "Zixiang Zhou",
            "Guozhen Zhang",
            "Youliang Zhang",
            "Yuan Zhou",
            "Qinglin Lu",
            "Yong-Jin Liu"
        ],
        "tldr": "StreamAvatar introduces a novel two-stage framework for real-time, interactive human avatar generation using streaming diffusion models, achieving state-of-the-art performance in quality, efficiency, and interaction naturalness with the help of several key components.",
        "tldr_zh": "StreamAvatar 提出了一个新颖的两阶段框架，利用流式扩散模型实现实时、交互式的人类化身生成，借助几个关键组件，在质量、效率和交互自然度方面都达到了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer",
        "summary": "Recent progress in diffusion models has significantly advanced the field of human image animation. While existing methods can generate temporally consistent results for short or regular motions, significant challenges remain, particularly in generating long-duration videos. Furthermore, the synthesis of fine-grained facial and hand details remains under-explored, limiting the applicability of current approaches in real-world, high-quality applications. To address these limitations, we propose a diffusion transformer (DiT)-based framework which focuses on generating high-fidelity and long-duration human animation videos. First, we design a set of hybrid implicit guidance signals and a sharpness guidance factor, enabling our framework to additionally incorporate detailed facial and hand features as guidance. Next, we incorporate the time-aware position shift fusion module, modify the input format within the DiT backbone, and refer to this mechanism as the Position Shift Adaptive Module, which enables video generation of arbitrary length. Finally, we introduce a novel data augmentation strategy and a skeleton alignment model to reduce the impact of human shape variations across different identities. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving superior performance in both high-fidelity and long-duration human image animation.",
        "url": "http://arxiv.org/abs/2512.21905v1",
        "published_date": "2025-12-26T07:36:48+00:00",
        "updated_date": "2025-12-26T07:36:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shen Zheng",
            "Jiaran Cai",
            "Yuansheng Guan",
            "Shenneng Huang",
            "Xingpei Ma",
            "Junjie Cao",
            "Hanfeng Zhao",
            "Qiang Zhang",
            "Shunsi Zhang",
            "Xiao-Ping Zhang"
        ],
        "tldr": "This paper introduces a Diffusion Transformer-based framework for high-fidelity and long-duration human image animation, addressing limitations in existing methods regarding detail synthesis and video length. It incorporates novel guidance signals, a time-aware module, and data augmentation techniques.",
        "tldr_zh": "本文介绍了一个基于扩散Transformer的框架，用于高保真和长时程人体图像动画，解决了现有方法在细节合成和视频长度方面的局限性。它结合了新的引导信号、时间感知模块和数据增强技术。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Inference-based GAN Video Generation",
        "summary": "Video generation has seen remarkable progresses thanks to advancements in generative deep learning. Generated videos should not only display coherent and continuous movement but also meaningful movement in successions of scenes. Generating models such as Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs) and more recently Diffusion Networks have been used for generating short video sequences, usually of up to 16 frames. In this paper, we first propose a new type of video generator by enabling adversarial-based unconditional video generators with a variational encoder, akin to a VAE-GAN hybrid structure, in order to enable the generation process with inference capabilities. The proposed model, as in other video deep learning-based processing frameworks, incorporates two processing branches, one for content and another for movement. However, existing models struggle with the temporal scaling of the generated videos. In classical approaches when aiming to increase the generated video length, the resulting video quality degrades, particularly when considering generating significantly long sequences. To overcome this limitation, our research study extends the initially proposed VAE-GAN video generation model by employing a novel, memory-efficient approach to generate long videos composed of hundreds or thousands of frames ensuring their temporal continuity, consistency and dynamics. Our approach leverages a Markov chain framework with a recall mechanism, with each state representing a VAE-GAN short-length video generator. This setup allows for the sequential connection of generated video sub-sequences, enabling temporal dependencies, resulting in meaningful long video sequences.",
        "url": "http://arxiv.org/abs/2512.21776v1",
        "published_date": "2025-12-25T20:14:38+00:00",
        "updated_date": "2025-12-25T20:14:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jingbo Yang",
            "Adrian G. Bors"
        ],
        "tldr": "This paper introduces a novel VAE-GAN hybrid model with a Markov chain framework and recall mechanism to generate long, temporally consistent videos by sequentially connecting short generated video subsequences.",
        "tldr_zh": "本文提出了一种新颖的VAE-GAN混合模型，该模型具有马尔可夫链框架和召回机制，通过顺序连接生成的短视频子序列来生成长时间、时间上一致的视频。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]