[
    {
        "title": "WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for Autonomous Driving",
        "summary": "Recent advances in driving-scene generation and reconstruction have\ndemonstrated significant potential for enhancing autonomous driving systems by\nproducing scalable and controllable training data. Existing generation methods\nprimarily focus on synthesizing diverse and high-fidelity driving videos;\nhowever, due to limited 3D consistency and sparse viewpoint coverage, they\nstruggle to support convenient and high-quality novel-view synthesis (NVS).\nConversely, recent 3D/4D reconstruction approaches have significantly improved\nNVS for real-world driving scenes, yet inherently lack generative capabilities.\nTo overcome this dilemma between scene generation and reconstruction, we\npropose \\textbf{WorldSplat}, a novel feed-forward framework for 4D\ndriving-scene generation. Our approach effectively generates consistent\nmulti-track videos through two key steps: ((i)) We introduce a 4D-aware latent\ndiffusion model integrating multi-modal information to produce pixel-aligned 4D\nGaussians in a feed-forward manner. ((ii)) Subsequently, we refine the novel\nview videos rendered from these Gaussians using a enhanced video diffusion\nmodel. Extensive experiments conducted on benchmark datasets demonstrate that\n\\textbf{WorldSplat} effectively generates high-fidelity, temporally and\nspatially consistent multi-track novel view driving videos.",
        "url": "http://arxiv.org/abs/2509.23402v1",
        "published_date": "2025-09-27T16:47:44+00:00",
        "updated_date": "2025-09-27T16:47:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyue Zhu",
            "Zhanqian Wu",
            "Zhenxin Zhu",
            "Lijun Zhou",
            "Haiyang Sun",
            "Bing Wan",
            "Kun Ma",
            "Guang Chen",
            "Hangjun Ye",
            "Jin Xie",
            "jian Yang"
        ]
    }
]