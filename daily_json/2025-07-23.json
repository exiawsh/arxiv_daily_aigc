[
    {
        "title": "PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation",
        "summary": "The rapid advancement of video diffusion models has been hindered by\nfundamental limitations in temporal modeling, particularly the rigid\nsynchronization of frame evolution imposed by conventional scalar timestep\nvariables. While task-specific adaptations and autoregressive models have\nsought to address these challenges, they remain constrained by computational\ninefficiency, catastrophic forgetting, or narrow applicability. In this work,\nwe present Pusa, a groundbreaking paradigm that leverages vectorized timestep\nadaptation (VTA) to enable fine-grained temporal control within a unified video\ndiffusion framework. Besides, VTA is a non-destructive adaptation, which means\nit fully preserves the capabilities of the base model. By finetuning the SOTA\nWan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency --\nsurpassing the performance of Wan-I2V-14B with $\\leq$ 1/200 of the training\ncost (\\$500 vs. $\\geq$ \\$100,000) and $\\leq$ 1/2500 of the dataset size (4K vs.\n$\\geq$ 10M samples). Pusa not only sets a new standard for image-to-video (I2V)\ngeneration, achieving a VBench-I2V total score of 87.32\\% (vs. 86.86\\% of\nWan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as\nstart-end frames and video extension -- all without task-specific training.\nMeanwhile, Pusa can still perform text-to-video generation. Mechanistic\nanalyses reveal that our approach preserves the foundation model's generative\npriors while surgically injecting temporal dynamics, avoiding the combinatorial\nexplosion inherent to vectorized timesteps. This work establishes a scalable,\nefficient, and versatile paradigm for next-generation video synthesis,\ndemocratizing high-fidelity video generation for research and industry alike.\nCode is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen",
        "url": "http://arxiv.org/abs/2507.16116v1",
        "published_date": "2025-07-22T00:09:37+00:00",
        "updated_date": "2025-07-22T00:09:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaofang Liu",
            "Yumeng Ren",
            "Aitor Artola",
            "Yuxuan Hu",
            "Xiaodong Cun",
            "Xiaotong Zhao",
            "Alan Zhao",
            "Raymond H. Chan",
            "Suiyun Zhang",
            "Rui Liu",
            "Dandan Tu",
            "Jean-Michel Morel"
        ],
        "tldr": "The paper introduces Pusa, a new video diffusion paradigm using vectorized timestep adaptation (VTA) that achieves SOTA I2V performance with significantly lower training costs and dataset size compared to existing models, while also enabling zero-shot multi-task capabilities.",
        "tldr_zh": "该论文介绍了 Pusa，一种新的视频扩散范式，使用向量化时间步长自适应 (VTA)，以远低于现有模型的训练成本和数据集大小实现了 SOTA I2V 性能，同时还实现了零样本多任务能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Controllable Video Generation: A Survey",
        "summary": "With the rapid development of AI-generated content (AIGC), video generation\nhas emerged as one of its most dynamic and impactful subfields. In particular,\nthe advancement of video generation foundation models has led to growing demand\nfor controllable video generation methods that can more accurately reflect user\nintent. Most existing foundation models are designed for text-to-video\ngeneration, where text prompts alone are often insufficient to express complex,\nmulti-modal, and fine-grained user requirements. This limitation makes it\nchallenging for users to generate videos with precise control using current\nmodels. To address this issue, recent research has explored the integration of\nadditional non-textual conditions, such as camera motion, depth maps, and human\npose, to extend pretrained video generation models and enable more controllable\nvideo synthesis. These approaches aim to enhance the flexibility and practical\napplicability of AIGC-driven video generation systems. In this survey, we\nprovide a systematic review of controllable video generation, covering both\ntheoretical foundations and recent advances in the field. We begin by\nintroducing the key concepts and commonly used open-source video generation\nmodels. We then focus on control mechanisms in video diffusion models,\nanalyzing how different types of conditions can be incorporated into the\ndenoising process to guide generation. Finally, we categorize existing methods\nbased on the types of control signals they leverage, including single-condition\ngeneration, multi-condition generation, and universal controllable generation.\nFor a complete list of the literature on controllable video generation\nreviewed, please visit our curated repository at\nhttps://github.com/mayuelala/Awesome-Controllable-Video-Generation.",
        "url": "http://arxiv.org/abs/2507.16869v1",
        "published_date": "2025-07-22T06:05:34+00:00",
        "updated_date": "2025-07-22T06:05:34+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Yue Ma",
            "Kunyu Feng",
            "Zhongyuan Hu",
            "Xinyu Wang",
            "Yucheng Wang",
            "Mingzhe Zheng",
            "Xuanhua He",
            "Chenyang Zhu",
            "Hongyu Liu",
            "Yingqing He",
            "Zeyu Wang",
            "Zhifeng Li",
            "Xiu Li",
            "Wei Liu",
            "Dan Xu",
            "Linfeng Zhang",
            "Qifeng Chen"
        ],
        "tldr": "This survey paper reviews the field of controllable video generation, focusing on methods that incorporate non-textual conditions like camera motion and pose to improve user control over generated videos, categorizing approaches based on the control signals used.",
        "tldr_zh": "该综述论文回顾了可控视频生成领域，重点介绍了结合非文本条件（如相机运动和姿势）以提高用户对生成视频控制的方法，并根据使用的控制信号对方法进行分类。",
        "relevance_score": 9,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation",
        "summary": "Existing text-to-video methods struggle to transfer motion smoothly from a\nreference object to a target object with significant differences in appearance\nor structure between them. To address this challenge, we introduce MotionShot,\na training-free framework capable of parsing reference-target correspondences\nin a fine-grained manner, thereby achieving high-fidelity motion transfer while\npreserving coherence in appearance. To be specific, MotionShot first performs\nsemantic feature matching to ensure high-level alignments between the reference\nand target objects. It then further establishes low-level morphological\nalignments through reference-to-target shape retargeting. By encoding motion\nwith temporal attention, our MotionShot can coherently transfer motion across\nobjects, even in the presence of significant appearance and structure\ndisparities, demonstrated by extensive experiments. The project page is\navailable at: https://motionshot.github.io/.",
        "url": "http://arxiv.org/abs/2507.16310v1",
        "published_date": "2025-07-22T07:51:05+00:00",
        "updated_date": "2025-07-22T07:51:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanchen Liu",
            "Yanan Sun",
            "Zhening Xing",
            "Junyao Gao",
            "Kai Chen",
            "Wenjie Pei"
        ],
        "tldr": "MotionShot is a training-free framework for text-to-video generation that addresses the challenge of transferring motion between objects with significant appearance and structural differences by using semantic feature matching and shape retargeting.",
        "tldr_zh": "MotionShot是一个无需训练的文本到视频生成框架，它通过语义特征匹配和形状重定向，解决了在外观和结构差异显著的物体之间传递运动的难题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]