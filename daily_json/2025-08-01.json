[
    {
        "title": "UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing",
        "summary": "In this paper, we propose UniLIP, which extends CLIP to reconstruction,\ngeneration and editing, thereby building a unified tokenizer upon its\nexceptional comprehension capabilities. Previous CLIP-based unified methods\noften require additional diffusion decoders or quantization to support\nreconstruction and generation tasks, leading to inconsistent reconstruction or\ndegradation of original comprehension performance.In contrast, we introduce a\ntwo-stage training scheme and a self-distillation strategy that progressively\nintegrates reconstruction capabilities into CLIP, allowing it to maintain\noriginal comprehension performance while achieving effective image\nreconstruction. Furthermore, we propose a dual-condition architecture to\nconnect the MLLM and diffusion transformer, using both learnable queries and\nthe last layer multimodal hidden states as joint conditions. This method not\nonly enables the utilization of the MLLM's strong reasoning capabilities in\ngeneration tasks, but also maximizes the exploitation of the rich information\nin UniLIP features during editing tasks. In text-to-image generation tasks,\nUniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark\nrespectively, surpassing all previous unified models of similar scale. In image\nediting, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark,\nsurpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP\neffectively expand the application scope of CLIP, enabling continuous CLIP\nfeatures to not only serve as the optimal choice for understanding tasks but\nalso achieve highly competitive performance in generation and editing tasks.",
        "url": "http://arxiv.org/abs/2507.23278v1",
        "published_date": "2025-07-31T06:35:03+00:00",
        "updated_date": "2025-07-31T06:35:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Tang",
            "Chenwei Xie",
            "Xiaoyi Bao",
            "Tingyu Weng",
            "Pandeng Li",
            "Yun Zheng",
            "Liwei Wang"
        ],
        "tldr": "UniLIP extends CLIP for unified multimodal understanding, generation, and editing using a two-stage training scheme and dual-condition architecture, achieving state-of-the-art performance in text-to-image generation and image editing.",
        "tldr_zh": "UniLIP扩展了CLIP，通过两阶段训练方案和双条件架构，实现了统一的多模态理解、生成和编辑，并在文本到图像生成和图像编辑方面取得了最先进的性能。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    },
    {
        "title": "PixNerd: Pixel Neural Field Diffusion",
        "summary": "The current success of diffusion transformers heavily depends on the\ncompressed latent space shaped by the pre-trained variational autoencoder(VAE).\nHowever, this two-stage training paradigm inevitably introduces accumulated\nerrors and decoding artifacts. To address the aforementioned problems,\nresearchers return to pixel space at the cost of complicated cascade pipelines\nand increased token complexity. In contrast to their efforts, we propose to\nmodel the patch-wise decoding with neural field and present a single-scale,\nsingle-stage, efficient, end-to-end solution, coined as pixel neural field\ndiffusion~(PixelNerd). Thanks to the efficient neural field representation in\nPixNerd, we directly achieved 2.15 FID on ImageNet $256\\times256$ and 2.84 FID\non ImageNet $512\\times512$ without any complex cascade pipeline or VAE. We also\nextend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16\nachieved a competitive 0.73 overall score on the GenEval benchmark and 80.9\noverall score on the DPG benchmark.",
        "url": "http://arxiv.org/abs/2507.23268v1",
        "published_date": "2025-07-31T06:07:20+00:00",
        "updated_date": "2025-07-31T06:07:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shuai Wang",
            "Ziteng Gao",
            "Chenhui Zhu",
            "Weilin Huang",
            "Limin Wang"
        ],
        "tldr": "PixNerd proposes a single-stage diffusion model using neural fields to directly generate images in pixel space, achieving competitive results on ImageNet and text-to-image generation without VAEs or cascade pipelines.",
        "tldr_zh": "PixNerd提出了一种使用神经场的单阶段扩散模型，可以直接在像素空间中生成图像，无需VAE或级联流水线即可在ImageNet和文本到图像生成方面取得有竞争力的结果。",
        "relevance_score": 3,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 5
    }
]