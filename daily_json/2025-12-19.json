[
    {
        "title": "Kling-Omni Technical Report",
        "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.",
        "url": "http://arxiv.org/abs/2512.16776v1",
        "published_date": "2025-12-18T17:08:12+00:00",
        "updated_date": "2025-12-18T17:08:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kling Team",
            "Jialu Chen",
            "Yuanzheng Ci",
            "Xiangyu Du",
            "Zipeng Feng",
            "Kun Gai",
            "Sainan Guo",
            "Feng Han",
            "Jingbin He",
            "Kang He",
            "Xiao Hu",
            "Xiaohua Hu",
            "Boyuan Jiang",
            "Fangyuan Kong",
            "Hang Li",
            "Jie Li",
            "Qingyu Li",
            "Shen Li",
            "Xiaohan Li",
            "Yan Li",
            "Jiajun Liang",
            "Borui Liao",
            "Yiqiao Liao",
            "Weihong Lin",
            "Quande Liu",
            "Xiaokun Liu",
            "Yilun Liu",
            "Yuliang Liu",
            "Shun Lu",
            "Hangyu Mao",
            "Yunyao Mao",
            "Haodong Ouyang",
            "Wenyu Qin",
            "Wanqi Shi",
            "Xiaoyu Shi",
            "Lianghao Su",
            "Haozhi Sun",
            "Peiqin Sun",
            "Pengfei Wan",
            "Chao Wang",
            "Chenyu Wang",
            "Meng Wang",
            "Qiulin Wang",
            "Runqi Wang",
            "Xintao Wang",
            "Xuebo Wang",
            "Zekun Wang",
            "Min Wei",
            "Tiancheng Wen",
            "Guohao Wu",
            "Xiaoshi Wu",
            "Zhenhua Wu",
            "Da Xie",
            "Yingtong Xiong",
            "Yulong Xu",
            "Sile Yang",
            "Zikang Yang",
            "Weicai Ye",
            "Ziyang Yuan",
            "Shenglong Zhang",
            "Shuaiyu Zhang",
            "Yuanxing Zhang",
            "Yufan Zhang",
            "Wenzheng Zhao",
            "Ruiliang Zhou",
            "Yan Zhou",
            "Guosheng Zhu",
            "Yongjie Zhu"
        ],
        "tldr": "Kling-Omni is a generalist generative framework that synthesizes high-fidelity videos from multimodal inputs, supporting various video generation, editing, and reasoning tasks within a unified system.",
        "tldr_zh": "Kling-Omni是一个通用生成框架，可以通过多模态输入合成高质量视频，并在统一的系统中支持各种视频生成、编辑和推理任务。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
        "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.",
        "url": "http://arxiv.org/abs/2512.16093v1",
        "published_date": "2025-12-18T02:21:30+00:00",
        "updated_date": "2025-12-18T02:21:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Jintao Zhang",
            "Kaiwen Zheng",
            "Kai Jiang",
            "Haoxu Wang",
            "Ion Stoica",
            "Joseph E. Gonzalez",
            "Jianfei Chen",
            "Jun Zhu"
        ],
        "tldr": "TurboDiffusion introduces a framework for significantly accelerating video diffusion models (100-200x speedup) using techniques like attention acceleration, step distillation, and quantization, while maintaining video quality.",
        "tldr_zh": "TurboDiffusion 提出了一种加速视频扩散模型的新框架，通过注意力加速、步骤提炼和量化等技术，在保持视频质量的同时，将视频生成速度提高 100-200 倍。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models",
        "summary": "State-of-the-art Text-to-Video (T2V) diffusion models can generate visually impressive results, yet they still frequently fail to compose complex scenes or follow logical temporal instructions. In this paper, we argue that many errors, including apparent motion failures, originate from the model's inability to construct a semantically correct or logically consistent initial frame. We introduce Factorized Video Generation (FVG), a pipeline that decouples these tasks by decomposing the Text-to-Video generation into three specialized stages: (1) Reasoning, where a Large Language Model (LLM) rewrites the video prompt to describe only the initial scene, resolving temporal ambiguities; (2) Composition, where a Text-to-Image (T2I) model synthesizes a high-quality, compositionally-correct anchor frame from this new prompt; and (3) Temporal Synthesis, where a video model, finetuned to understand this anchor, focuses its entire capacity on animating the scene and following the prompt. Our decomposed approach sets a new state-of-the-art on the T2V CompBench benchmark and significantly improves all tested models on VBench2. Furthermore, we show that visual anchoring allows us to cut the number of sampling steps by 70% without any loss in performance, leading to a substantial speed-up in sampling. Factorized Video Generation offers a simple yet practical path toward more efficient, robust, and controllable video synthesis",
        "url": "http://arxiv.org/abs/2512.16371v1",
        "published_date": "2025-12-18T10:10:45+00:00",
        "updated_date": "2025-12-18T10:10:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mariam Hassan",
            "Bastien Van Delft",
            "Wuyang Li",
            "Alexandre Alahi"
        ],
        "tldr": "The paper introduces Factorized Video Generation (FVG), a three-stage pipeline that decomposes Text-to-Video generation into reasoning, composition (using a T2I model to create an anchor frame), and temporal synthesis, achieving state-of-the-art results and significant speed-up in sampling.",
        "tldr_zh": "该论文介绍了分解视频生成（FVG），一个三阶段流程，将文本到视频的生成分解为推理、构图（使用T2I模型创建锚帧）和时间合成，实现了最先进的结果，并显著加快了采样速度。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion",
        "summary": "We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.",
        "url": "http://arxiv.org/abs/2512.16023v1",
        "published_date": "2025-12-17T23:16:02+00:00",
        "updated_date": "2025-12-17T23:16:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liudi Yang",
            "Yang Bai",
            "George Eskandar",
            "Fengyi Shen",
            "Mohammad Altillawi",
            "Dong Chen",
            "Ziyuan Liu",
            "Abhinav Valada"
        ],
        "tldr": "The paper introduces CoVAR, a multi-modal diffusion model for co-generating video and robot actions from text instructions and initial states, outperforming existing methods by leveraging pre-trained video knowledge and a novel Bridge Attention mechanism.",
        "tldr_zh": "该论文介绍了CoVAR，一种多模态扩散模型，用于从文本指令和初始状态共同生成视频和机器人动作，通过利用预训练的视频知识和一种新颖的桥接注意力机制，优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spatia: Video Generation with Updatable Spatial Memory",
        "summary": "Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.",
        "url": "http://arxiv.org/abs/2512.15716v1",
        "published_date": "2025-12-17T18:59:59+00:00",
        "updated_date": "2025-12-17T18:59:59+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jinjing Zhao",
            "Fangyun Wei",
            "Zhening Liu",
            "Hongyang Zhang",
            "Chang Xu",
            "Yan Lu"
        ],
        "tldr": "Spatia is a video generation framework that uses a 3D point cloud spatial memory, updated through visual SLAM, to improve long-term spatial and temporal consistency in generated videos and enable camera control and 3D-aware editing.",
        "tldr_zh": "Spatia是一个视频生成框架，它使用通过视觉SLAM更新的3D点云空间记忆，以提高生成视频中长期空间和时间的一致性，并实现相机控制和3D感知编辑。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "End-to-End Training for Autoregressive Video Diffusion via Self-Resampling",
        "summary": "Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.",
        "url": "http://arxiv.org/abs/2512.15702v1",
        "published_date": "2025-12-17T18:53:29+00:00",
        "updated_date": "2025-12-17T18:53:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuwei Guo",
            "Ceyuan Yang",
            "Hao He",
            "Yang Zhao",
            "Meng Wei",
            "Zhenheng Yang",
            "Weilin Huang",
            "Dahua Lin"
        ],
        "tldr": "This paper introduces Resampling Forcing, a novel end-to-end training framework for autoregressive video diffusion models that addresses exposure bias through a self-resampling scheme and history routing, achieving comparable performance to distillation methods with better temporal consistency in long videos.",
        "tldr_zh": "该论文介绍了一种名为Resampling Forcing的全新端到端训练框架，用于自回归视频扩散模型，通过自重采样方案和历史路由解决暴露偏差问题，在长视频中实现了与蒸馏方法相当的性能，并具有更好的时间一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]