[
    {
        "title": "Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis",
        "summary": "In recent years, large text-to-video (T2V) synthesis models have garnered\nconsiderable attention for their abilities to generate videos from textual\ndescriptions. However, achieving both high imaging quality and effective motion\nrepresentation remains a significant challenge for these T2V models. Existing\napproaches often adapt pre-trained text-to-image (T2I) models to refine video\nframes, leading to issues such as flickering and artifacts due to\ninconsistencies across frames. In this paper, we introduce EVS, a training-free\nEncapsulated Video Synthesizer that composes T2I and T2V models to enhance both\nvisual fidelity and motion smoothness of generated videos. Our approach\nutilizes a well-trained diffusion-based T2I model to refine low-quality video\nframes by treating them as out-of-distribution samples, effectively optimizing\nthem with noising and denoising steps. Meanwhile, we employ T2V backbones to\nensure consistent motion dynamics. By encapsulating the T2V temporal-only prior\ninto the T2I generation process, EVS successfully leverages the strengths of\nboth types of models, resulting in videos of improved imaging and motion\nquality. Experimental results validate the effectiveness of our approach\ncompared to previous approaches. Our composition process also leads to a\nsignificant improvement of 1.6x-4.5x speedup in inference time. Source codes:\nhttps://github.com/Tonniia/EVS.",
        "url": "http://arxiv.org/abs/2507.13753v1",
        "published_date": "2025-07-18T08:59:02+00:00",
        "updated_date": "2025-07-18T08:59:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tongtong Su",
            "Chengyu Wang",
            "Bingyan Liu",
            "Jun Huang",
            "Dongming Lu"
        ],
        "tldr": "The paper introduces EVS, a training-free method for high-quality video synthesis by composing text-to-image and text-to-video models, improving visual fidelity and motion smoothness while also accelerating inference.",
        "tldr_zh": "该论文介绍了一种名为EVS的免训练方法，通过组合文本到图像和文本到视频模型来合成高质量视频，从而提高视觉保真度和运动平滑度，并加速推理过程。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "$\\nabla$NABLA: Neighborhood Adaptive Block-Level Attention",
        "summary": "Recent progress in transformer-based architectures has demonstrated\nremarkable success in video generation tasks. However, the quadratic complexity\nof full attention mechanisms remains a critical bottleneck, particularly for\nhigh-resolution and long-duration video sequences. In this paper, we propose\nNABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that\ndynamically adapts to sparsity patterns in video diffusion transformers (DiTs).\nBy leveraging block-wise attention with adaptive sparsity-driven threshold,\nNABLA reduces computational overhead while preserving generative quality. Our\nmethod does not require custom low-level operator design and can be seamlessly\nintegrated with PyTorch's Flex Attention operator. Experiments demonstrate that\nNABLA achieves up to 2.7x faster training and inference compared to baseline\nalmost without compromising quantitative metrics (CLIP score, VBench score,\nhuman evaluation score) and visual quality drop. The code and model weights are\navailable here: https://github.com/gen-ai-team/Wan2.1-NABLA",
        "url": "http://arxiv.org/abs/2507.13546v1",
        "published_date": "2025-07-17T21:36:36+00:00",
        "updated_date": "2025-07-17T21:36:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dmitrii Mikhailov",
            "Aleksey Letunovskiy",
            "Maria Kovaleva",
            "Vladimir Arkhipkin",
            "Vladimir Korviakov",
            "Vladimir Polovnikov",
            "Viacheslav Vasilev",
            "Evelina Sidorova",
            "Denis Dimitrov"
        ],
        "tldr": "The paper introduces NABLA, a novel block-level attention mechanism for video diffusion transformers that adaptively adjusts sparsity to improve efficiency without significant quality loss, offering up to 2.7x speedup.",
        "tldr_zh": "该论文介绍了一种名为NABLA的新型块级注意力机制，用于视频扩散transformer，该机制自适应地调整稀疏性以提高效率，且不会显著降低质量，可实现高达2.7倍的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]