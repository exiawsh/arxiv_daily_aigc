[
    {
        "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation",
        "summary": "Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.",
        "url": "http://arxiv.org/abs/2508.00782v1",
        "published_date": "2025-08-01T17:05:04+00:00",
        "updated_date": "2025-08-01T17:05:04+00:00",
        "categories": [
            "cs.GR",
            "cs.AI",
            "cs.CV",
            "cs.MM",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Kien T. Pham",
            "Yingqing He",
            "Yazhou Xing",
            "Qifeng Chen",
            "Long Chen"
        ],
        "tldr": "The paper introduces SpA2V, a framework that leverages spatial auditory cues to generate videos with better semantic and spatial alignment to input audio, using a MLLM to create Video Scene Layouts and a diffusion model for video generation.",
        "tldr_zh": "该论文介绍了SpA2V，一个利用空间听觉线索生成视频的框架，该框架能更好地将视频的语义和空间与输入音频对齐。它使用 MLLM 创建视频场景布局，并使用扩散模型生成视频。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models",
        "summary": "In the recent development of conditional diffusion models still require heavy\nsupervised fine-tuning for performing control on a category of tasks.\nTraining-free conditioning via guidance with off-the-shelf models is a\nfavorable alternative to avoid further fine-tuning on the base model. However,\nthe existing training-free guidance frameworks either have heavy memory\nrequirements or offer sub-optimal control due to rough estimation. These\nshortcomings limit the applicability to control diffusion models that require\nintense computation, such as Text-to-Video (T2V) diffusion models. In this\nwork, we propose Taming Inference Time Alignment for Guided Text-to-Video\nDiffusion Model, so-called TITAN-Guide, which overcomes memory space issues,\nand provides more optimal control in the guidance process compared to the\ncounterparts. In particular, we develop an efficient method for optimizing\ndiffusion latents without backpropagation from a discriminative guiding model.\nIn particular, we study forward gradient descents for guided diffusion tasks\nwith various options on directional directives. In our experiments, we\ndemonstrate the effectiveness of our approach in efficiently managing memory\nduring latent optimization, while previous methods fall short. Our proposed\napproach not only minimizes memory requirements but also significantly enhances\nT2V performance across a range of diffusion guidance benchmarks. Code, models,\nand demo are available at https://titanguide.github.io.",
        "url": "http://arxiv.org/abs/2508.00289v1",
        "published_date": "2025-08-01T03:26:18+00:00",
        "updated_date": "2025-08-01T03:26:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Christian Simon",
            "Masato Ishii",
            "Akio Hayakawa",
            "Zhi Zhong",
            "Shusuke Takahashi",
            "Takashi Shibuya",
            "Yuki Mitsufuji"
        ],
        "tldr": "The paper introduces TITAN-Guide, a memory-efficient and control-optimized method for training-free text-to-video diffusion model guidance that optimizes diffusion latents without backpropagation, enhancing T2V performance.",
        "tldr_zh": "该论文介绍了TITAN-Guide，一种内存高效且控制优化的文本到视频扩散模型无训练引导方法，该方法无需反向传播即可优化扩散潜在变量，从而提高T2V性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]