[
    {
        "title": "Yume: An Interactive World Generation Model",
        "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
        "url": "http://arxiv.org/abs/2507.17744v1",
        "published_date": "2025-07-23T17:57:09+00:00",
        "updated_date": "2025-07-23T17:57:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.HC"
        ],
        "authors": [
            "Xiaofeng Mao",
            "Shaoheng Lin",
            "Zhen Li",
            "Chuanhao Li",
            "Wenshuo Peng",
            "Tong He",
            "Jiangmiao Pang",
            "Mingmin Chi",
            "Yu Qiao",
            "Kaipeng Zhang"
        ],
        "tldr": "Yume is a model for generating interactive and dynamic worlds from images using a novel framework including camera motion quantization, a Masked Video Diffusion Transformer, an advanced sampler, and model acceleration techniques, with code and data publicly available.",
        "tldr_zh": "Yume是一个基于图像生成交互式和动态世界的模型，它采用了一个新颖的框架，包括相机运动量化、Masked Video Diffusion Transformer、高级采样器和模型加速技术，并公开发布了代码和数据。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EndoGen: Conditional Autoregressive Endoscopic Video Generation",
        "summary": "Endoscopic video generation is crucial for advancing medical imaging and\nenhancing diagnostic capabilities. However, prior efforts in this field have\neither focused on static images, lacking the dynamic context required for\npractical applications, or have relied on unconditional generation that fails\nto provide meaningful references for clinicians. Therefore, in this paper, we\npropose the first conditional endoscopic video generation framework, namely\nEndoGen. Specifically, we build an autoregressive model with a tailored\nSpatiotemporal Grid-Frame Patterning (SGP) strategy. It reformulates the\nlearning of generating multiple frames as a grid-based image generation\npattern, which effectively capitalizes the inherent global dependency modeling\ncapabilities of autoregressive architectures. Furthermore, we propose a\nSemantic-Aware Token Masking (SAT) mechanism, which enhances the model's\nability to produce rich and diverse content by selectively focusing on\nsemantically meaningful regions during the generation process. Through\nextensive experiments, we demonstrate the effectiveness of our framework in\ngenerating high-quality, conditionally guided endoscopic content, and improves\nthe performance of downstream task of polyp segmentation. Code released at\nhttps://www.github.com/CUHK-AIM-Group/EndoGen.",
        "url": "http://arxiv.org/abs/2507.17388v1",
        "published_date": "2025-07-23T10:32:20+00:00",
        "updated_date": "2025-07-23T10:32:20+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Xinyu Liu",
            "Hengyu Liu",
            "Cheng Wang",
            "Tianming Liu",
            "Yixuan Yuan"
        ],
        "tldr": "EndoGen is a conditional autoregressive framework for endoscopic video generation, using spatiotemporal grid-frame patterning and semantic-aware token masking to improve video quality and polyp segmentation performance.",
        "tldr_zh": "EndoGen是一个内窥镜视频生成的条件自回归框架，它使用时空网格帧模式和语义感知令牌掩蔽来提高视频质量和息肉分割性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]