[
    {
        "title": "EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory",
        "summary": "Humans possess a remarkable ability to mentally explore and replay 3D\nenvironments they have previously experienced. Inspired by this mental process,\nwe present EvoWorld: a world model that bridges panoramic video generation with\nevolving 3D memory to enable spatially consistent long-horizon exploration.\nGiven a single panoramic image as input, EvoWorld first generates future video\nframes by leveraging a video generator with fine-grained view control, then\nevolves the scene's 3D reconstruction using a feedforward plug-and-play\ntransformer, and finally synthesizes futures by conditioning on geometric\nreprojections from this evolving explicit 3D memory. Unlike prior\nstate-of-the-arts that synthesize videos only, our key insight lies in\nexploiting this evolving 3D reconstruction as explicit spatial guidance for the\nvideo generation process, projecting the reconstructed geometry onto target\nviewpoints to provide rich spatial cues that significantly enhance both visual\nrealism and geometric consistency. To evaluate long-range exploration\ncapabilities, we introduce the first comprehensive benchmark spanning synthetic\noutdoor environments, Habitat indoor scenes, and challenging real-world\nscenarios, with particular emphasis on loop-closure detection and spatial\ncoherence over extended trajectories. Extensive experiments demonstrate that\nour evolving 3D memory substantially improves visual fidelity and maintains\nspatial scene coherence compared to existing approaches, representing a\nsignificant advance toward long-horizon spatially consistent world modeling.",
        "url": "http://arxiv.org/abs/2510.01183v1",
        "published_date": "2025-10-01T17:59:38+00:00",
        "updated_date": "2025-10-01T17:59:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Wang",
            "Luoxin Ye",
            "TaiMing Lu",
            "Junfei Xiao",
            "Jiahan Zhang",
            "Yuxiang Guo",
            "Xijun Liu",
            "Rama Chellappa",
            "Cheng Peng",
            "Alan Yuille",
            "Jieneng Chen"
        ],
        "tldr": "EvoWorld presents a world model that generates long-horizon, spatially consistent videos by evolving a 3D representation of the scene, which is then used to guide video generation. It introduces a benchmark for evaluating long-range exploration.",
        "tldr_zh": "EvoWorld 提出了一个世界模型，通过演化场景的 3D 表示来生成长时程、空间一致的视频，然后利用该 3D 表示来指导视频生成。 它还引入了一个用于评估长距离探索的基准。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation",
        "summary": "Current video generation models produce physically inconsistent motion that\nviolates real-world dynamics. We propose TrajVLM-Gen, a two-stage framework for\nphysics-aware image-to-video generation. First, we employ a Vision Language\nModel to predict coarse-grained motion trajectories that maintain consistency\nwith real-world physics. Second, these trajectories guide video generation\nthrough attention-based mechanisms for fine-grained motion refinement. We build\na trajectory prediction dataset based on video tracking data with realistic\nmotion patterns. Experiments on UCF-101 and MSR-VTT demonstrate that\nTrajVLM-Gen outperforms existing methods, achieving competitive FVD scores of\n545 on UCF-101 and 539 on MSR-VTT.",
        "url": "http://arxiv.org/abs/2510.00806v1",
        "published_date": "2025-10-01T12:11:36+00:00",
        "updated_date": "2025-10-01T12:11:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fan Yang",
            "Zhiyang Chen",
            "Yousong Zhu",
            "Xin Li",
            "Jinqiao Wang"
        ],
        "tldr": "The paper introduces TrajVLM-Gen, a two-stage vision-language framework for physics-aware video generation, using trajectory prediction to guide motion refinement and improve consistency with real-world dynamics, demonstrating improvements on UCF-101 and MSR-VTT datasets.",
        "tldr_zh": "该论文提出了TrajVLM-Gen，一个用于物理感知视频生成的两阶段视觉-语言框架，它使用轨迹预测来指导运动细化，并提高与现实世界动态的一致性，并在UCF-101和MSR-VTT数据集上展示了改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration",
        "summary": "Diffusion Transformer has shown remarkable abilities in generating\nhigh-fidelity videos, delivering visually coherent frames and rich details over\nextended durations. However, existing video generation models still fall short\nin subject-consistent video generation due to an inherent difficulty in parsing\nprompts that specify complex spatial relationships, temporal logic, and\ninteractions among multiple subjects. To address this issue, we propose\nBindWeave, a unified framework that handles a broad range of subject-to-video\nscenarios from single-subject cases to complex multi-subject scenes with\nheterogeneous entities. To bind complex prompt semantics to concrete visual\nsubjects, we introduce an MLLM-DiT framework in which a pretrained multimodal\nlarge language model performs deep cross-modal reasoning to ground entities and\ndisentangle roles, attributes, and interactions, yielding subject-aware hidden\nstates that condition the diffusion transformer for high-fidelity\nsubject-consistent video generation. Experiments on the OpenS2V benchmark\ndemonstrate that our method achieves superior performance across subject\nconsistency, naturalness, and text relevance in generated videos, outperforming\nexisting open-source and commercial models.",
        "url": "http://arxiv.org/abs/2510.00438v1",
        "published_date": "2025-10-01T02:41:11+00:00",
        "updated_date": "2025-10-01T02:41:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhaoyang Li",
            "Dongjun Qian",
            "Kai Su",
            "Qishuai Diao",
            "Xiangyang Xia",
            "Chang Liu",
            "Wenfei Yang",
            "Tianzhu Zhang",
            "Zehuan Yuan"
        ],
        "tldr": "BindWeave introduces a new MLLM-DiT framework for subject-consistent video generation, using cross-modal reasoning to improve consistency, naturalness, and text relevance in generated videos, particularly in complex multi-subject scenarios.",
        "tldr_zh": "BindWeave 提出了一个新的 MLLM-DiT 框架，用于生成主题一致的视频。该框架利用跨模态推理来提高生成视频的一致性、自然性和文本相关性，尤其是在复杂的多主题场景中。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
        "summary": "While recent generative models advance pixel-space video synthesis, they\nremain limited in producing professional educational videos, which demand\ndisciplinary knowledge, precise visual structures, and coherent transitions,\nlimiting their applicability in educational scenarios. Intuitively, such\nrequirements are better addressed through the manipulation of a renderable\nenvironment, which can be explicitly controlled via logical commands (e.g.,\ncode). In this work, we propose Code2Video, a code-centric agent framework for\ngenerating educational videos via executable Python code. The framework\ncomprises three collaborative agents: (i) Planner, which structures lecture\ncontent into temporally coherent flows and prepares corresponding visual\nassets; (ii) Coder, which converts structured instructions into executable\nPython codes while incorporating scope-guided auto-fix to enhance efficiency;\nand (iii) Critic, which leverages vision-language models (VLM) with visual\nanchor prompts to refine spatial layout and ensure clarity. To support\nsystematic evaluation, we build MMMC, a benchmark of professionally produced,\ndiscipline-specific educational videos. We evaluate MMMC across diverse\ndimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and\nparticularly, TeachQuiz, a novel end-to-end metric that quantifies how well a\nVLM, after unlearning, can recover knowledge by watching the generated videos.\nOur results demonstrate the potential of Code2Video as a scalable,\ninterpretable, and controllable approach, achieving 40% improvement over direct\ncode generation and producing videos comparable to human-crafted tutorials. The\ncode and datasets are available at https://github.com/showlab/Code2Video.",
        "url": "http://arxiv.org/abs/2510.01174v1",
        "published_date": "2025-10-01T17:56:48+00:00",
        "updated_date": "2025-10-01T17:56:48+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.HC",
            "cs.MM"
        ],
        "authors": [
            "Yanzhe Chen",
            "Kevin Qinghong Lin",
            "Mike Zheng Shou"
        ],
        "tldr": "The paper introduces Code2Video, a code-centric framework for generating educational videos using executable Python code and three collaborative agents: Planner, Coder, and Critic. It demonstrates promising results on a new benchmark, MMMC, indicating its potential as a scalable and controllable approach.",
        "tldr_zh": "本文介绍了 Code2Video，一个以代码为中心的框架，用于通过可执行的 Python 代码和三个协同代理（Planner、Coder 和 Critic）生成教育视频。 它在一个新的基准 MMMC 上展示了有希望的结果，表明它作为一种可扩展和可控的方法的潜力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation",
        "summary": "Audio-video generation has often relied on complex multi-stage architectures\nor sequential synthesis of sound and visuals. We introduce Ovi, a unified\nparadigm for audio-video generation that models the two modalities as a single\ngenerative process. By using blockwise cross-modal fusion of twin-DiT modules,\nOvi achieves natural synchronization and removes the need for separate\npipelines or post hoc alignment. To facilitate fine-grained multimodal fusion\nmodeling, we initialize an audio tower with an architecture identical to that\nof a strong pretrained video model. Trained from scratch on hundreds of\nthousands of hours of raw audio, the audio tower learns to generate realistic\nsound effects, as well as speech that conveys rich speaker identity and\nemotion. Fusion is obtained by jointly training the identical video and audio\ntowers via blockwise exchange of timing (via scaled-RoPE embeddings) and\nsemantics (through bidirectional cross-attention) on a vast video corpus. Our\nmodel enables cinematic storytelling with natural speech and accurate,\ncontext-matched sound effects, producing movie-grade video clips. All the\ndemos, code and model weights are published at https://aaxwaz.github.io/Ovi",
        "url": "http://arxiv.org/abs/2510.01284v1",
        "published_date": "2025-09-30T21:03:50+00:00",
        "updated_date": "2025-09-30T21:03:50+00:00",
        "categories": [
            "cs.MM",
            "cs.CV",
            "cs.SD",
            "eess.AS"
        ],
        "authors": [
            "Chetwin Low",
            "Weimin Wang",
            "Calder Katyal"
        ],
        "tldr": "Ovi is a novel audio-video generation model utilizing twin-DiT modules with blockwise cross-modal fusion, trained from scratch to produce high-quality, synchronized audio and video. The model allows for cinematic storytelling with natural speech and accurate sound effects.",
        "tldr_zh": "Ovi 是一种新颖的音视频生成模型，它利用具有分块跨模态融合的 twin-DiT 模块，从头开始训练以生成高质量、同步的音频和视频。该模型可以通过自然的语音和准确的声音效果实现电影级的叙事。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]