[
    {
        "title": "MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation",
        "summary": "Video generation has recently shown superiority in urban scene synthesis for\nautonomous driving. Existing video generation approaches to autonomous driving\nprimarily focus on RGB video generation and lack the ability to support\nmulti-modal video generation. However, multi-modal data, such as depth maps and\nsemantic maps, are crucial for holistic urban scene understanding in autonomous\ndriving. Although it is feasible to use multiple models to generate different\nmodalities, this increases the difficulty of model deployment and does not\nleverage complementary cues for multi-modal data generation. To address this\nproblem, in this work, we propose a novel multi-modal multi-view video\ngeneration approach to autonomous driving. Specifically, we construct a unified\ndiffusion transformer model composed of modal-shared components and\nmodal-specific components. Then, we leverage diverse conditioning inputs to\nencode controllable scene structure and content cues into the unified diffusion\nmodel for multi-modal multi-view video generation. In this way, our approach is\ncapable of generating multi-modal multi-view driving scene videos in a unified\nframework. Our experiments on the challenging real-world autonomous driving\ndataset, nuScenes, show that our approach can generate multi-modal multi-view\nurban scene videos with high fidelity and controllability, surpassing the\nstate-of-the-art methods.",
        "url": "http://arxiv.org/abs/2508.14327v1",
        "published_date": "2025-08-20T00:51:36+00:00",
        "updated_date": "2025-08-20T00:51:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guile Wu",
            "David Huang",
            "Dongfeng Bai",
            "Bingbing Liu"
        ],
        "tldr": "This paper introduces MoVieDrive, a novel multi-modal multi-view video generation approach for autonomous driving using a unified diffusion transformer model, demonstrating improved fidelity and controllability on the nuScenes dataset.",
        "tldr_zh": "该论文介绍了MoVieDrive，一种新颖的多模态多视角视频生成方法，用于自动驾驶，它使用统一的扩散Transformer模型，并在nuScenes数据集上展示了改进的逼真度和可控性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]