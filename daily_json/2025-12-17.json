[
    {
        "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling",
        "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.",
        "url": "http://arxiv.org/abs/2512.14614v1",
        "published_date": "2025-12-16T17:22:46+00:00",
        "updated_date": "2025-12-16T17:22:46+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Wenqiang Sun",
            "Haiyu Zhang",
            "Haoyuan Wang",
            "Junta Wu",
            "Zehan Wang",
            "Zhenwei Wang",
            "Yunhong Wang",
            "Jun Zhang",
            "Tengfei Wang",
            "Chunchao Guo"
        ],
        "tldr": "WorldPlay is a real-time video diffusion model for interactive world modeling that achieves long-term geometric consistency using a dual action representation, reconstituted context memory, and context forcing, generating 720p video at 24 FPS.",
        "tldr_zh": "WorldPlay是一个用于交互式世界建模的实时视频扩散模型，它通过双重动作表示、重构上下文记忆和上下文强制来实现长期几何一致性，从而生成24 FPS的720p视频。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "SS4D: Native 4D Generative Model via Structured Spacetime Latents",
        "summary": "We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion",
        "url": "http://arxiv.org/abs/2512.14284v1",
        "published_date": "2025-12-16T10:45:06+00:00",
        "updated_date": "2025-12-16T10:45:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhibing Li",
            "Mengchen Zhang",
            "Tong Wu",
            "Jing Tan",
            "Jiaqi Wang",
            "Dahua Lin"
        ],
        "tldr": "SS4D presents a native 4D generative model for synthesizing dynamic 3D objects from monocular video, achieving high fidelity and temporal consistency by directly training on 4D data with structured spacetime latents.",
        "tldr_zh": "SS4D 提出了一种原生的 4D 生成模型，用于从单目视频合成动态 3D 对象。该模型通过直接在 4D 数据上训练，并利用结构化的时空潜在变量，实现了高保真度和时间一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos",
        "summary": "Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.",
        "url": "http://arxiv.org/abs/2512.14217v1",
        "published_date": "2025-12-16T09:11:36+00:00",
        "updated_date": "2025-12-16T09:11:36+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yang Bai",
            "Liudi Yang",
            "George Eskandar",
            "Fengyi Shen",
            "Mohammad Altillawi",
            "Ziyuan Liu",
            "Gitta Kutyniok"
        ],
        "tldr": "DRAW2ACT is a depth-aware trajectory-conditioned video generation framework that creates robotic demonstration videos using a diffusion model conditioned on depth, semantics, shape, and motion, and demonstrates superior visual fidelity and manipulation success rates.",
        "tldr_zh": "DRAW2ACT是一个深度感知的轨迹条件视频生成框架，它使用一个以深度、语义、形状和运动为条件的扩散模型来创建机器人演示视频，并展示了卓越的视觉保真度和操作成功率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling",
        "summary": "Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.",
        "url": "http://arxiv.org/abs/2512.14056v1",
        "published_date": "2025-12-16T03:49:52+00:00",
        "updated_date": "2025-12-16T03:49:52+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kim Sung-Bin",
            "Joohyun Chang",
            "David Harwath",
            "Tae-Hyun Oh"
        ],
        "tldr": "The paper presents FacEDiT, a unified framework for talking face editing and generation based on speech-conditional facial motion infilling using a Diffusion Transformer, and introduces a new benchmark dataset, FacEDiTBench.",
        "tldr_zh": "该论文提出了 FacEDiT，一个基于语音条件面部运动填充的统一框架，用于说话人脸编辑和生成，使用 Diffusion Transformer，并引入了一个新的基准数据集 FacEDiTBench。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]