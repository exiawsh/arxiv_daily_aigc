[
    {
        "title": "Captain Cinema: Towards Short Movie Generation",
        "summary": "We present Captain Cinema, a generation framework for short movie generation.\nGiven a detailed textual description of a movie storyline, our approach firstly\ngenerates a sequence of keyframes that outline the entire narrative, which\nensures long-range coherence in both the storyline and visual appearance (e.g.,\nscenes and characters). We refer to this step as top-down keyframe planning.\nThese keyframes then serve as conditioning signals for a video synthesis model,\nwhich supports long context learning, to produce the spatio-temporal dynamics\nbetween them. This step is referred to as bottom-up video synthesis. To support\nstable and efficient generation of multi-scene long narrative cinematic works,\nwe introduce an interleaved training strategy for Multimodal Diffusion\nTransformers (MM-DiT), specifically adapted for long-context video data. Our\nmodel is trained on a specially curated cinematic dataset consisting of\ninterleaved data pairs. Our experiments demonstrate that Captain Cinema\nperforms favorably in the automated creation of visually coherent and narrative\nconsistent short movies in high quality and efficiency. Project page:\nhttps://thecinema.ai",
        "url": "http://arxiv.org/abs/2507.18634v1",
        "published_date": "2025-07-24T17:59:56+00:00",
        "updated_date": "2025-07-24T17:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junfei Xiao",
            "Ceyuan Yang",
            "Lvmin Zhang",
            "Shengqu Cai",
            "Yang Zhao",
            "Yuwei Guo",
            "Gordon Wetzstein",
            "Maneesh Agrawala",
            "Alan Yuille",
            "Lu Jiang"
        ],
        "tldr": "Captain Cinema is a framework for short movie generation from textual descriptions, using top-down keyframe planning and bottom-up video synthesis with Multimodal Diffusion Transformers.",
        "tldr_zh": "Captain Cinema是一个从文本描述生成短电影的框架，它使用自顶向下的关键帧规划和自底向上的视频合成，并结合了多模态扩散Transformer。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis",
        "summary": "Distribution Matching Distillation (DMD) is a promising score distillation\ntechnique that compresses pre-trained teacher diffusion models into efficient\none-step or multi-step student generators. Nevertheless, its reliance on the\nreverse Kullback-Leibler (KL) divergence minimization potentially induces mode\ncollapse (or mode-seeking) in certain applications. To circumvent this inherent\ndrawback, we propose Adversarial Distribution Matching (ADM), a novel framework\nthat leverages diffusion-based discriminators to align the latent predictions\nbetween real and fake score estimators for score distillation in an adversarial\nmanner. In the context of extremely challenging one-step distillation, we\nfurther improve the pre-trained generator by adversarial distillation with\nhybrid discriminators in both latent and pixel spaces. Different from the mean\nsquared error used in DMD2 pre-training, our method incorporates the\ndistributional loss on ODE pairs collected from the teacher model, and thus\nproviding a better initialization for score distillation fine-tuning in the\nnext stage. By combining the adversarial distillation pre-training with ADM\nfine-tuning into a unified pipeline termed DMDX, our proposed method achieves\nsuperior one-step performance on SDXL compared to DMD2 while consuming less GPU\ntime. Additional experiments that apply multi-step ADM distillation on\nSD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient\nimage and video synthesis.",
        "url": "http://arxiv.org/abs/2507.18569v1",
        "published_date": "2025-07-24T16:45:05+00:00",
        "updated_date": "2025-07-24T16:45:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yanzuo Lu",
            "Yuxi Ren",
            "Xin Xia",
            "Shanchuan Lin",
            "Xing Wang",
            "Xuefeng Xiao",
            "Andy J. Ma",
            "Xiaohua Xie",
            "Jian-Huang Lai"
        ],
        "tldr": "This paper introduces Adversarial Distribution Matching (ADM), a novel framework for distilling diffusion models, addressing mode collapse issues in existing Distribution Matching Distillation (DMD) techniques, and achieving state-of-the-art performance in efficient image and video synthesis.",
        "tldr_zh": "本文提出了一种新的对抗分布匹配（ADM）框架，用于蒸馏扩散模型，解决了现有分布匹配蒸馏（DMD）技术中的模式崩溃问题，并在高效图像和视频合成方面取得了最先进的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Scene Transition Awareness in Video Generation via Post-Training",
        "summary": "Recent advances in AI-generated video have shown strong performance on\n\\emph{text-to-video} tasks, particularly for short clips depicting a single\nscene. However, current models struggle to generate longer videos with coherent\nscene transitions, primarily because they cannot infer when a transition is\nneeded from the prompt. Most open-source models are trained on datasets\nconsisting of single-scene video clips, which limits their capacity to learn\nand respond to prompts requiring multiple scenes. Developing scene transition\nawareness is essential for multi-scene generation, as it allows models to\nidentify and segment videos into distinct clips by accurately detecting\ntransitions.\n  To address this, we propose the \\textbf{Transition-Aware Video} (TAV)\ndataset, which consists of preprocessed video clips with multiple scene\ntransitions. Our experiment shows that post-training on the \\textbf{TAV}\ndataset improves prompt-based scene transition understanding, narrows the gap\nbetween required and generated scenes, and maintains image quality.",
        "url": "http://arxiv.org/abs/2507.18046v1",
        "published_date": "2025-07-24T02:50:26+00:00",
        "updated_date": "2025-07-24T02:50:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hanwen Shen",
            "Jiajie Lu",
            "Yupeng Cao",
            "Xiaonan Yang"
        ],
        "tldr": "This paper introduces a Transition-Aware Video (TAV) dataset and demonstrates that post-training video generation models on this dataset improves their ability to generate videos with coherent scene transitions from text prompts.",
        "tldr_zh": "本文介绍了一个Transition-Aware Video (TAV)数据集，并展示了通过在该数据集上对视频生成模型进行后训练，可以提高模型根据文本提示生成具有连贯场景过渡视频的能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image",
        "summary": "Advances in generative modeling have significantly enhanced digital content\ncreation, extending from 2D images to complex 3D and 4D scenes. Despite\nsubstantial progress, producing high-fidelity and temporally consistent dynamic\n4D content remains a challenge. In this paper, we propose MVG4D, a novel\nframework that generates dynamic 4D content from a single still image by\ncombining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,\nMVG4D employs an image matrix module that synthesizes temporally coherent and\nspatially diverse multi-view images, providing rich supervisory signals for\ndownstream 3D and 4D reconstruction. These multi-view images are used to\noptimize a 3D Gaussian point cloud, which is further extended into the temporal\ndomain via a lightweight deformation network. Our method effectively enhances\ntemporal consistency, geometric fidelity, and visual realism, addressing key\nchallenges in motion discontinuity and background degradation that affect prior\n4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate\nthat MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and\ntime efficiency. Notably, it reduces flickering artifacts and sharpens\nstructural details across views and time, enabling more immersive AR/VR\nexperiences. MVG4D sets a new direction for efficient and controllable 4D\ngeneration from minimal inputs.",
        "url": "http://arxiv.org/abs/2507.18371v2",
        "published_date": "2025-07-24T12:48:14+00:00",
        "updated_date": "2025-07-31T11:48:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "DongFu Yin",
            "Xiaotian Chen",
            "Fei Richard Yu",
            "Xuanchen Li",
            "Xinhao Zhang"
        ],
        "tldr": "MVG4D generates temporally consistent 4D content from a single image by combining multi-view synthesis with 4D Gaussian Splatting, outperforming existing methods in fidelity, consistency, and time efficiency.",
        "tldr_zh": "MVG4D结合多视角合成和4D高斯溅射，从单张图像生成时间上一致的4D内容，在保真度、一致性和时间效率方面优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]