[
    {
        "title": "PAN: A World Model for General, Interactable, and Long-Horizon World Simulation",
        "summary": "A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.",
        "url": "http://arxiv.org/abs/2511.09057v1",
        "published_date": "2025-11-12T07:20:35+00:00",
        "updated_date": "2025-11-13T01:29:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "PAN Team",
            "Jiannan Xiang",
            "Yi Gu",
            "Zihan Liu",
            "Zeyu Feng",
            "Qiyue Gao",
            "Yiyan Hu",
            "Benhao Huang",
            "Guangyi Liu",
            "Yichi Yang",
            "Kun Zhou",
            "Davit Abrahamyan",
            "Arif Ahmad",
            "Ganesh Bannur",
            "Junrong Chen",
            "Kimi Chen",
            "Mingkai Deng",
            "Ruobing Han",
            "Xinqi Huang",
            "Haoqiang Kang",
            "Zheqi Li",
            "Enze Ma",
            "Hector Ren",
            "Yashowardhan Shinde",
            "Rohan Shingre",
            "Ramsundar Tanikella",
            "Kaiming Tao",
            "Dequan Yang",
            "Xinle Yu",
            "Cong Zeng",
            "Binglin Zhou",
            "Hector Liu",
            "Zhiting Hu",
            "Eric P. Xing"
        ],
        "tldr": "PAN is a general world model using a LLM-backed latent dynamics model and a video diffusion decoder for generating interactive and long-horizon video simulations conditioned on language actions, demonstrating strong performance in various tasks.",
        "tldr_zh": "PAN是一个通用世界模型，它使用由LLM支持的潜在动态模型和视频扩散解码器来生成以语言动作为条件的交互式和长时程视频模拟，并在各种任务中表现出强大的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
        "summary": "The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.",
        "url": "http://arxiv.org/abs/2511.08585v1",
        "published_date": "2025-11-11T18:59:50+00:00",
        "updated_date": "2025-11-12T02:05:57+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jingtong Yue",
            "Ziqi Huang",
            "Zhaoxi Chen",
            "Xintao Wang",
            "Pengfei Wan",
            "Ziwei Liu"
        ],
        "tldr": "This paper surveys the evolution of video generation towards video foundation models that serve as implicit world models, simulating physical dynamics and agent interactions, and outlines future directions.",
        "tldr_zh": "该论文综述了视频生成向作为隐式世界模型的视频基础模型的演变，这些模型模拟物理动力学和智能体交互，并概述了未来的发展方向。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    }
]