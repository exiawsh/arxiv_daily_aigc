[
    {
        "title": "Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention",
        "summary": "Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose \\textsc{Light Forcing}, the \\textit{first} sparse attention solution tailored for AR video generation models. It incorporates a \\textit{Chunk-Aware Growth} mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a \\textit{Hierarchical Sparse Attention} to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\\eg, 84.5 on VBench) and efficiency (\\eg, $1.2{\\sim}1.3\\times$ end-to-end speedup). Combined with FP8 quantization and LightVAE, \\textsc{Light Forcing} further achieves a $2.3\\times$ speedup and 19.7\\,FPS on an RTX~5090 GPU. Code will be released at \\href{https://github.com/chengtao-lv/LightForcing}{https://github.com/chengtao-lv/LightForcing}.",
        "url": "http://arxiv.org/abs/2602.04789v1",
        "published_date": "2026-02-04T17:41:53+00:00",
        "updated_date": "2026-02-04T17:41:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengtao Lv",
            "Yumeng Shi",
            "Yushi Huang",
            "Ruihao Gong",
            "Shen Ren",
            "Wenya Wang"
        ],
        "tldr": "The paper introduces Light Forcing, a sparse attention mechanism tailored for autoregressive video diffusion models, which improves efficiency and quality by using Chunk-Aware Growth and Hierarchical Sparse Attention. It achieves significant speedups and maintains high visual fidelity.",
        "tldr_zh": "本文介绍了Light Forcing，一种为自回归视频扩散模型定制的稀疏注意力机制，通过使用Chunk-Aware Growth和分层稀疏注意力来提高效率和质量。该方法实现了显著的加速，同时保持了较高的视觉保真度。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Adaptive 1D Video Diffusion Autoencoder",
        "summary": "Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.",
        "url": "http://arxiv.org/abs/2602.04220v1",
        "published_date": "2026-02-04T05:11:12+00:00",
        "updated_date": "2026-02-04T05:11:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yao Teng",
            "Minxuan Lin",
            "Xian Liu",
            "Shuai Wang",
            "Xiao Yang",
            "Xihui Liu"
        ],
        "tldr": "This paper introduces One-DVA, a transformer-based video autoencoder with adaptive compression and diffusion-based decoding, aiming to overcome limitations of existing video autoencoders, with performance comparable to 3D-CNN VAEs and support for higher compression ratios.",
        "tldr_zh": "本文介绍了一种基于Transformer的视频自动编码器One-DVA，它具有自适应压缩和基于扩散的解码，旨在克服现有视频自动编码器的局限性，其性能与3D-CNN VAE相当，并支持更高的压缩率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VTok: A Unified Video Tokenizer with Decoupled Spatial-Temporal Latents",
        "summary": "This work presents VTok, a unified video tokenization framework that can be used for both generation and understanding tasks. Unlike the leading vision-language systems that tokenize videos through a naive frame-sampling strategy, we propose to decouple the spatial and temporal representations of videos by retaining the spatial features of a single key frame while encoding each subsequent frame into a single residual token, achieving compact yet expressive video tokenization. Our experiments suggest that VTok effectively reduces the complexity of video representation from the product of frame count and per-frame token count to their sum, while the residual tokens sufficiently capture viewpoint and motion changes relative to the key frame. Extensive evaluations demonstrate the efficacy and efficiency of VTok: it achieves notably higher performance on a range of video understanding and text-to-video generation benchmarks compared with baselines using naive tokenization, all with shorter token sequences per video (e.g., 3.4% higher accuracy on our TV-Align benchmark and 1.9% higher VBench score). Remarkably, VTok produces more coherent motion and stronger guidance following in text-to-video generation, owing to its more consistent temporal encoding. We hope VTok can serve as a standardized video tokenization paradigm for future research in video understanding and generation.",
        "url": "http://arxiv.org/abs/2602.04202v1",
        "published_date": "2026-02-04T04:39:46+00:00",
        "updated_date": "2026-02-04T04:39:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feng Wang",
            "Yichun Shi",
            "Ceyuan Yang",
            "Qiushan Guo",
            "Jingxiang Sun",
            "Alan Yuille",
            "Peng Wang"
        ],
        "tldr": "The paper introduces VTok, a unified video tokenization framework that decouples spatial and temporal representations, achieving efficient and effective video encoding for both understanding and generation tasks.",
        "tldr_zh": "该论文介绍了VTok，一个统一的视频标记化框架，通过解耦空间和时间表示，实现了高效且有效的视频编码，适用于理解和生成任务。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PrevizWhiz: Combining Rough 3D Scenes and 2D Video to Guide Generative Video Previsualization",
        "summary": "In pre-production, filmmakers and 3D animation experts must rapidly prototype ideas to explore a film's possibilities before fullscale production, yet conventional approaches involve trade-offs in efficiency and expressiveness. Hand-drawn storyboards often lack spatial precision needed for complex cinematography, while 3D previsualization demands expertise and high-quality rigged assets. To address this gap, we present PrevizWhiz, a system that leverages rough 3D scenes in combination with generative image and video models to create stylized video previews. The workflow integrates frame-level image restyling with adjustable resemblance, time-based editing through motion paths or external video inputs, and refinement into high-fidelity video clips. A study with filmmakers demonstrates that our system lowers technical barriers for film-makers, accelerates creative iteration, and effectively bridges the communication gap, while also surfacing challenges of continuity, authorship, and ethical consideration in AI-assisted filmmaking.",
        "url": "http://arxiv.org/abs/2602.03838v1",
        "published_date": "2026-02-03T18:56:40+00:00",
        "updated_date": "2026-02-03T18:56:40+00:00",
        "categories": [
            "cs.HC",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Erzhen Hu",
            "Frederik Brudy",
            "David Ledo",
            "George Fitzmaurice",
            "Fraser Anderson"
        ],
        "tldr": "PrevizWhiz combines rough 3D scenes and generative AI to create stylized video previews for filmmakers, addressing the limitations of traditional pre-production methods.",
        "tldr_zh": "PrevizWhiz 结合粗略的 3D 场景和生成式人工智能，为电影制作人创建风格化的视频预览，从而解决了传统预制作方法的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]