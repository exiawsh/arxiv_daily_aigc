[
    {
        "title": "Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing",
        "summary": "Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.",
        "url": "http://arxiv.org/abs/2602.09609v1",
        "published_date": "2026-02-10T10:01:16+00:00",
        "updated_date": "2026-02-10T10:01:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jialun Liu",
            "Yukuo Ma",
            "Xiao Cao",
            "Tian Li",
            "Gonghu Shang",
            "Haibin Huang",
            "Chi Zhang",
            "Xuelong Li",
            "Cong Liu",
            "Junqi Liu",
            "Jiakui Hu",
            "Robby T. Tan",
            "Shiwen Zhang",
            "Liying Yang",
            "Xiaoyan Yang",
            "Qizhen Weng",
            "Xiangzhen Chang",
            "Yuanzhi Liang",
            "Yifan Xu",
            "Zhiyong Huang",
            "Zuoxin Li",
            "Xuelong Li"
        ],
        "tldr": "The paper introduces Tele-Omni, a unified multimodal framework for video generation and editing that supports various input modalities (text, images, reference videos) and tasks by leveraging pretrained large language models for instruction parsing and diffusion models for video synthesis.",
        "tldr_zh": "该论文介绍了一个名为 Tele-Omni 的统一多模态框架，用于视频生成和编辑。它支持多种输入模态（文本、图像、参考视频）并通过利用预训练的大型语言模型进行指令解析和扩散模型进行视频合成来执行多种任务。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures",
        "summary": "Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel Plücker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.",
        "url": "http://arxiv.org/abs/2602.09600v1",
        "published_date": "2026-02-10T09:51:07+00:00",
        "updated_date": "2026-02-10T09:51:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxi Wang",
            "Wenqi Ouyang",
            "Tianyi Wei",
            "Yi Dong",
            "Zhiqi Shen",
            "Xingang Pan"
        ],
        "tldr": "Hand2World introduces an autoregressive framework for generating photorealistic egocentric videos of hand-object interactions, addressing challenges in monocular view ambiguity, distribution shift, and long-term stability by using 3D hand meshes and Plücker-ray embeddings for camera geometry injection.",
        "tldr_zh": "Hand2World 提出了一个自回归框架，用于生成逼真的手部与物体交互的自我中心视角视频，通过使用 3D 手部网格和 Plücker 射线嵌入来解决单目视图歧义、分布偏移和长期稳定性等挑战，实现相机几何注入。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models",
        "summary": "This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.",
        "url": "http://arxiv.org/abs/2602.09022v1",
        "published_date": "2026-02-09T18:59:47+00:00",
        "updated_date": "2026-02-09T18:59:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zehan Wang",
            "Tengfei Wang",
            "Haiyu Zhang",
            "Xuhui Zuo",
            "Junta Wu",
            "Haoyuan Wang",
            "Wenqiang Sun",
            "Zhenwei Wang",
            "Chenjie Cao",
            "Hengshuang Zhao",
            "Chunchao Guo",
            "Zhou Zhao"
        ],
        "tldr": "WorldCompass is a reinforcement learning framework that enhances long-horizon, video-based world models like WorldPlay through interaction signals, significantly improving interaction accuracy and visual fidelity using clip-level rollouts, complementary rewards, and efficient RL techniques.",
        "tldr_zh": "WorldCompass是一个强化学习框架，通过交互信号增强了像WorldPlay这样的长时程、基于视频的世界模型，利用片段级别展开、互补奖励和高效的强化学习技术，显著提高了交互准确性和视觉保真度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]